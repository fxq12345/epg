import os
import gzip
import requests
import time
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor
from lxml import etree
import requests.adapters
from requests.packages.urllib3.util.retry import Retry

# å…¨å±€é…ç½®
OUTPUT_DIR = "output"
MAX_RETRY = 3
TIMEOUT = 30

def create_session():
    session = requests.Session()
    retry_strategy = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET"]
    )
    adapter = requests.adapters.HTTPAdapter(
        max_retries=retry_strategy,
        pool_connections=10,
        pool_maxsize=10
    )
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    return session

def fetch_with_retry(url):
    session = create_session()
    retry_cnt = 0
    while retry_cnt < MAX_RETRY:
        retry_cnt += 1
        try:
            print(f"ğŸ”„ æŠ“å–: {url[:60]}... ç¬¬{retry_cnt}æ¬¡")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            resp = session.get(url, timeout=TIMEOUT, headers=headers, stream=True)
            resp.raise_for_status()
            content = resp.content
            
            try_gzip = False
            if url.endswith(".gz"):
                try_gzip = True
            elif resp.headers.get("content-encoding") == "gzip":
                try_gzip = True
            elif resp.headers.get("Content-Type", "").endswith("gzip"):
                try_gzip = True
            
            if try_gzip:
                try:
                    content = gzip.decompress(content)
                    print(f"  æ£€æµ‹åˆ°gzipæ ¼å¼ï¼Œå·²è§£å‹")
                except (gzip.BadGzipFile, OSError):
                    print(f"  è­¦å‘Šï¼šæ ‡è®°ä¸ºgzipä½†å®é™…ä¸æ˜¯ï¼ŒæŒ‰æ™®é€šXMLå¤„ç†")
            
            try:
                xml_str = content.decode('utf-8')
            except UnicodeDecodeError:
                try:
                    xml_str = content.decode('gbk')
                except:
                    xml_str = content.decode('utf-8', errors='ignore')
            
            tree = etree.fromstring(xml_str.encode('utf-8'))
            ch = len(tree.findall(".//channel"))
            pg = len(tree.findall(".//programme"))
            print(f"âœ… æˆåŠŸ: é¢‘é“ {ch} èŠ‚ç›® {pg}")
            return True, tree, ch, pg
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ ç½‘ç»œé”™è¯¯: {type(e).__name__}: {str(e)[:80]}")
        except etree.XMLSyntaxError as e:
            print(f"âŒ XMLè§£æé”™è¯¯: {str(e)[:80]}")
        except Exception as e:
            print(f"âŒ å…¶ä»–é”™è¯¯: {type(e).__name__}: {str(e)[:80]}")
        
        if retry_cnt < MAX_RETRY:
            time.sleep(2 ** retry_cnt)
    
    return False, None, 0, 0

def merge_all(local_file):
    all_channels = []
    all_programs = []

    # è¯»å–å¹¶å»é‡URL
    with open("config.txt", "r", encoding="utf-8") as f:
        urls = list({line.strip() for line in f if line.strip().startswith("http")})

    print(f"ğŸ“¥ ç½‘ç»œæºå…± {len(urls)} ä¸ª")

    xml_trees = []
    with ThreadPoolExecutor(max_workers=5) as executor:
        tasks = {executor.submit(fetch_with_retry, u): u for u in urls}
        for t in tasks:
            ok, tree, ch, pg = t.result()
            if ok and tree is not None:
                xml_trees.append(tree)

    print(f"ğŸ“¥ æˆåŠŸåŠ è½½ {len(xml_trees)} ä¸ªXML")

    # ç»Ÿä¸€é¢‘é“IDä¸ºåç§°
    id_map = {}
    for tree in xml_trees:
        for ch in tree.findall(".//channel"):
            cid = ch.get("id", "").strip()
            dn = ch.find("display-name")
            name = dn.text.strip() if (dn is not None and dn.text) else cid
            if cid and name and cid not in id_map:
                id_map[cid] = name

    exist_names = set()
    for tree in xml_trees:
        for ch in tree.findall(".//channel"):
            old_id = ch.get("id", "").strip()
            name = id_map.get(old_id, old_id)
            if name and name not in exist_names:
                exist_names.add(name)
                ch.set("id", name)
                all_channels.append(ch)

    for tree in xml_trees:
        for p in tree.findall(".//programme"):
            old_c = p.get("channel", "").strip()
            new_c = id_map.get(old_c, old_c)
            if new_c:
                p.set("channel", new_c)
            tit = p.find("title")
            if tit is None or not tit.text or len(tit.text.strip()) < 1:
                continue
            all_programs.append(p)

    # åˆå¹¶æ½åŠæœ¬åœ°æº
    if os.path.exists(local_file):
        try:
            with gzip.open(local_file, "rb") as f:
                local_tree = etree.fromstring(f.read())
            local_map = {}
            for ch in local_tree.findall(".//channel"):
                cid = ch.get("id", "").strip()
                dn = ch.find("display-name")
                name = dn.text.strip() if (dn is not None and dn.text) else cid
                local_map[cid] = name
                if name and name not in exist_names:
                    exist_names.add(name)
                    ch.set("id", name)
                    all_channels.append(ch)
            for p in local_tree.findall(".//programme"):
                old_c = p.get("channel", "").strip()
                new_c = local_map.get(old_c, old_c)
                if new_c:
                    p.set("channel", new_c)
                tit = p.find("title")
                if tit is None or not tit.text or len(tit.text.strip()) < 1:
                    continue
                all_programs.append(p)
            print("âœ… æ½åŠæœ¬åœ°4é¢‘é“å·²åˆå¹¶")
        except Exception as e:
            print(f"âš ï¸ æ½åŠæºè¯»å–å¤±è´¥ï¼Œå·²è·³è¿‡: {e}")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°æ½åŠæºæ–‡ä»¶ {local_file}ï¼Œå·²è·³è¿‡")

    # é¢‘é“æ’åºï¼šå±±ä¸œ > æ½åŠ > CCTV > å«è§† > å…¶ä»–
    def channel_sort_key(channel_elem):
        name = channel_elem.get("id", "").strip()
        if "å±±ä¸œ" in name:
            return 0, name
        elif "æ½åŠ" in name:
            return 1, name
        elif "CCTV" in name:
            return 2, name
        elif "å«è§†" in name:
            return 3, name
        else:
            return 99, name

    all_channels.sort(key=channel_sort_key)

    # èŠ‚ç›®å»é‡ + æ—¶é—´èŒƒå›´ï¼šæ”¾å®½è¿‡æ»¤ç‰ˆ
    print(f"åŸå§‹èŠ‚ç›®æ•°: {len(all_programs)}")
    unique = []
    seen = set()

    now = datetime.now()
    # ====================== å·²æ”¾å®½æ—¶é—´èŒƒå›´ ======================
    start_cutoff = datetime(now.year, now.month, now.day, 0, 0, 0) - timedelta(days=3)   # ä»å‰2å¤© â†’ å‰3å¤©
    end_cutoff   = datetime(now.year, now.month, now.day, 23, 59, 59) + timedelta(days=8) # ä»æœªæ¥7å¤© â†’ æœªæ¥8å¤©

    for p in all_programs:
        try:
            key = p.get("channel") + "|" + p.get("start")
            if key in seen:
                continue

            # è¿‡æ»¤ç©ºæ ‡é¢˜ï¼ˆå·²æ”¾å®½ï¼šâ‰¥1å­—å³å¯ï¼‰
            title_elem = p.find("title")
            title = title_elem.text.strip() if (title_elem is not None and title_elem.text) else ""
            if not title or len(title) < 1:
                continue

            # å…¼å®¹ 14 ä½æ—¶é—´ï¼ˆé˜²æ­¢æˆªæ–­å‡ºé”™ï¼‰
            start_str = p.get("start", "")[:14]
            if len(start_str) >= 12:
                p_start = datetime.strptime(start_str[:12], "%Y%m%d%H%M")
            else:
                continue

            # æ—¶é—´è¿‡æ»¤ï¼ˆå·²æ”¾å®½ï¼‰
            if not (start_cutoff <= p_start <= end_cutoff):
                continue

            seen.add(key)
            unique.append(p)

        except Exception as e:
            # æ‰“å°å¼‚å¸¸èŠ‚ç›®ï¼Œæ–¹ä¾¿ä½ æ’æŸ¥
            # print(f"âš ï¸ èŠ‚ç›®å¼‚å¸¸: {p.get('channel')} {p.get('start')} â†’ {e}")
            continue

    unique.sort(key=lambda x: (x.get("channel", ""), x.get("start", "")))
    print(f"å»é‡åèŠ‚ç›®: {len(unique)}")

    # è¾“å‡º
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    out_path = os.path.join(OUTPUT_DIR, "epg.gz")

    root = etree.Element("tv")
    root.insert(0, etree.Comment(f"Built {datetime.now()} | æ”¾å®½è¿‡æ»¤ç‰ˆï¼šä¿ç•™å‰3å¤©+æœªæ¥8å¤©"))
    for ch in all_channels:
        root.append(ch)
    for p in unique:
        root.append(p)

    xml_data = etree.tostring(root, encoding="utf-8", pretty_print=True, xml_declaration=True)
    with gzip.open(out_path, "wb") as f:
        f.write(xml_data)

    size = os.path.getsize(out_path) / 1024 / 1024
    print("="*60)
    print(f"âœ… æœ€ç»ˆç”Ÿæˆå®Œæˆï¼ï¼ˆå·²æ”¾å®½è¿‡æ»¤ï¼‰")
    print(f"ğŸ“… æ—¶é—´èŒƒå›´ï¼š{start_cutoff.strftime('%Y-%m-%d')} è‡³ {end_cutoff.strftime('%Y-%m-%d')}")
    print(f"ğŸ“º é¢‘é“æ€»æ•°ï¼š{len(all_channels)}")
    print(f"ğŸ“… æœ‰æ•ˆèŠ‚ç›®ï¼š{len(unique)}")
    print(f"ğŸ“¦ æ–‡ä»¶å¤§å°ï¼š{size:.2f}MB")
    print("="*60)

if __name__ == "__main__":
    merge_all("weifang.gz")
