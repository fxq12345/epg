import os
import gzip
import re
import time
import signal
import requests
from lxml import etree
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor
from collections import defaultdict

# 10åˆ†é’Ÿå¼ºåˆ¶ç»ˆæ­¢
signal.signal(signal.SIGALRM, lambda s, f: os._exit(0))
signal.alarm(600)

OUTPUT_DIR = "output"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ====================== æ½åŠ4é¢‘é“é…ç½® ======================
WEIFANG_CHANNELS = [
    ("æ½åŠæ–°é—»é¢‘é“", "https://m.tvsou.com/epg/db502561"),
    ("æ½åŠç»æµç”Ÿæ´»é¢‘é“", "https://m.tvsou.com/epg/47a9d24a"),
    ("æ½åŠç§‘æ•™é¢‘é“", "https://m.tvsou.com/epg/d131d3d1"),
    ("æ½åŠå…¬å…±é¢‘é“", "https://m.tvsou.com/epg/c06f0cc0")
]

# ç½‘ç«™å›ºå®šåç¼€ï¼šå‘¨ä¸€w1 ~ å‘¨æ—¥w7
WEEK_MAP = {
    "å‘¨ä¸€": "w1",
    "å‘¨äºŒ": "w2",
    "å‘¨ä¸‰": "w3",
    "å‘¨å››": "w4",
    "å‘¨äº”": "w5",
    "å‘¨å…­": "w6",
    "å‘¨æ—¥": "w7"
}

MAX_RETRY = 2

# === å¿…åº”Referer é˜²åçˆ¬ ===
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Linux; Android 12; Mobile) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Mobile Safari/537.36",
    "Referer": "https://www.bing.com/search?q=%E7%94%B5%E8%A7%86%E8%8A%82%E7%9B%AE%E8%A1%A8"
}

# --- å¯é€‰Seleniumï¼ˆä¸è£…ä¹Ÿèƒ½è·‘ï¼‰---
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

# ====================== XMLä¿®å¤å‡½æ•° ======================
def clean_xml_content(content):
    """å½»åº•æ¸…æ´—XMLå†…å®¹ï¼Œä¿®å¤æ ¼å¼é”™è¯¯"""
    if not content:
        return ""
    
    # 1. ä¿®å¤é”™è¯¯çš„é—­åˆæ ‡ç­¾
    content = re.sub(r'<//title>', '</title>', content)
    content = re.sub(r'</></title>', '</title>', content)
    
    # 2. ä¿®å¤å±æ€§å€¼æ¢è¡Œ
    content = re.sub(r'(start|stop|channel)=\s*\n\s*"([^"]+)"', r'\1="\2"', content)
    
    # 3. ä¿®å¤å­¤ç«‹çš„<title>æ ‡ç­¾
    lines = content.split('\n')
    cleaned_lines = []
    i = 0
    while i < len(lines):
        line = lines[i]
        stripped = line.strip()
        
        # ä¿®å¤å­¤ç«‹çš„<title>æ ‡ç­¾
        if stripped == '<title>' and (i+1 >= len(lines) or not lines[i+1].strip().startswith('</title>')):
            # æŸ¥æ‰¾ä¸‹ä¸€ä¸ª</programme>æˆ–<programme>
            j = i + 1
            while j < len(lines) and not lines[j].strip().startswith(('</programme>', '<programme')):
                j += 1
            # åœ¨åˆé€‚ä½ç½®æ’å…¥</title>
            if j < len(lines):
                lines.insert(j, '  </title>')
        
        cleaned_lines.append(line)
        i += 1
    
    content = '\n'.join(cleaned_lines)
    
    # 4. ä¿®å¤ä¸åŒ¹é…çš„</programme>
    content = re.sub(r'</programme>\s*<programme', '</programme>\n<programme', content)
    
    # 5. ç§»é™¤æ²¡æœ‰å¯¹åº”å¼€æ ‡ç­¾çš„</programme>
    programme_open = 0
    lines = content.split('\n')
    cleaned_lines = []
    
    for line in lines:
        stripped = line.strip()
        if stripped.startswith('<programme'):
            programme_open += 1
            cleaned_lines.append(line)
        elif stripped == '</programme>':
            if programme_open > 0:
                programme_open -= 1
                cleaned_lines.append(line)
            # å¦åˆ™è·³è¿‡è¿™ä¸ªå¤šä½™çš„</programme>
        else:
            cleaned_lines.append(line)
    
    return '\n'.join(cleaned_lines)

# ====================== å·¥å…·å‡½æ•° ======================
def time_to_xmltv(base_date, time_str):
    try:
        hh, mm = time_str.strip().split(":")
        dt = datetime.combine(base_date, datetime.min.time().replace(hour=int(hh), minute=int(mm)))
        return dt.strftime("%Y%m%d%H%M%S +0800")
    except:
        return ""

def get_page_html(url):
    try:
        resp = requests.get(url, headers=HEADERS, timeout=12)
        resp.encoding = 'utf-8'
        html_content = resp.text
        if re.findall(r'\d{1,2}:\d{2}', html_content):
            return html_content
    except Exception:
        pass

    if SELENIUM_AVAILABLE:
        try:
            opt = Options()
            opt.add_argument("--headless")
            opt.add_argument("--no-sandbox")
            opt.add_argument("--disable-dev-shm-usage")
            opt.add_argument(f"user-agent={HEADERS['User-Agent']}")
            driver = webdriver.Chrome(options=opt)
            driver.get(url)
            time.sleep(2.5)
            html_content = driver.page_source
            driver.quit()
            return html_content
        except Exception:
            pass
    return ""

# ====================== æ ¸å¿ƒï¼šæŠ“ã€æœ¬å‘¨ä¸€ ~ å‘¨æ—¥ã€‘7å¤© ======================
def get_channel_7days(channel_name, base_url):
    week_list = list(WEEK_MAP.items())
    today = datetime.now()
    monday = today - timedelta(days=today.weekday())
    channel_progs = []

    for i, (week_name, w_suffix) in enumerate(week_list):
        current_date = monday + timedelta(days=i)

        if base_url.endswith('/'):
            url = f"{base_url}{w_suffix}"
        else:
            url = f"{base_url}/{w_suffix}"

        html_content = get_page_html(url)
        if not html_content:
            time.sleep(1)
            continue

        soup = BeautifulSoup(html_content, "html.parser")
        items = soup.find_all("div", class_=re.compile("program-item|time-item", re.I))
        if not items:
            items = soup.find_all("li")

        day_progs = []
        for item in items:
            txt = item.get_text(strip=True)
            match = re.search(r'(\d{1,2}:\d{2})\s*(.+)', txt)
            if not match:
                continue
            t_str, title = match.groups()
            if len(title) < 2 or 'å¹¿å‘Š' in title or 'æŠ¥æ—¶' in title:
                continue
            day_progs.append((t_str.strip(), title.strip()))

        day_progs = sorted(list(set(day_progs)), key=lambda x: x[0])
        for idx in range(len(day_progs)):
            t_start, title = day_progs[idx]
            if idx < len(day_progs)-1:
                t_end = day_progs[idx+1][0]
            else:
                h, m = map(int, t_start.split(':'))
                end_dt = datetime(2000, 1, 1, h, m) + timedelta(minutes=30)
                t_end = end_dt.strftime("%H:%M")

            start = time_to_xmltv(current_date, t_start)
            end = time_to_xmltv(current_date, t_end)
            if start and end:
                channel_progs.append((start, end, title))
        time.sleep(1.0)
    return channel_progs

# ====================== æ½åŠ7å¤©æŠ“å– ======================
def crawl_weifang():
    try:
        root = etree.Element("tv")
        for ch_name, _ in WEIFANG_CHANNELS:
            ch = etree.SubElement(root, "channel", id=ch_name)
            dn = etree.SubElement(ch, "display-name", lang="zh")
            dn.text = ch_name

        for ch_name, base_url in WEIFANG_CHANNELS:
            programs = get_channel_7days(channel_name=ch_name, base_url=base_url)
            for start, stop, title in programs:
                prog = etree.SubElement(root, "programme", start=start, stop=stop, channel=ch_name)
                t = etree.SubElement(prog, "title", lang="zh")
                t.text = title

        wf_path = os.path.join(OUTPUT_DIR, "weifang.gz")
        xml_content = etree.tostring(root, encoding="utf-8", pretty_print=True, xml_declaration=True)
        
        with gzip.open(wf_path, "wb") as f:
            f.write(xml_content)
            
        print(f"âœ… æ½åŠEPGå·²ä¿å­˜: {wf_path}")
        return wf_path
        
    except Exception as e:
        print(f"âŒ æ½åŠæºæŠ“å–å¤±è´¥: {e}")
        wf_path = os.path.join(OUTPUT_DIR, "weifang.gz")
        empty_xml = b'<?xml version="1.0" encoding="utf-8"?>\n<tv></tv>'
        with gzip.open(wf_path, "wb") as f:
            f.write(empty_xml)
        return wf_path

# ====================== æ”¹è¿›çš„åˆå¹¶é€»è¾‘ ======================
def fetch_with_retry(u, max_retry=MAX_RETRY):
    for attempt in range(1, max_retry + 1):
        try:
            r = requests.get(u, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
            if r.status_code not in (200, 206):
                time.sleep(1)
                continue

            if u.endswith(".gz"):
                content = gzip.decompress(r.content).decode("utf-8", "ignore")
            else:
                content = r.text

            # æ¸…ç†XMLå†…å®¹
            content = clean_xml_content(content)
            
            # ä¿®å¤å¸¸è§æ ¼å¼é—®é¢˜
            content = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f]', '', content)
            content = content.replace("& ", "&amp; ")
            
            # å°è¯•è§£æ
            try:
                parser = etree.XMLParser(recover=True)
                tree = etree.fromstring(content.encode("utf-8"), parser=parser)
                
                # éªŒè¯åŸºæœ¬ç»“æ„
                channels = tree.xpath("//channel")
                programmes = tree.xpath("//programme")
                
                if len(channels) > 0 and len(programmes) > 0:
                    return (True, tree, len(channels), len(programmes), attempt)
            except Exception as e:
                print(f"âš ï¸ XMLè§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤: {e}")
                # å°è¯•æå–æœ‰æ•ˆæ•°æ®
                pass
                
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥ {u[:50]}...: {e}")
            time.sleep(1)
            continue
    return (False, None, 0, 0, max_retry)

def merge_all(weifang_gz_file):
    all_channels = []
    all_programs = []
    total_ch = 0
    total_pg = 0
    success_cnt = 0
    fail_cnt = 0

    if not os.path.exists("config.txt"):
        print("âŒ æœªæ‰¾åˆ° config.txt æ–‡ä»¶")
        return

    with open("config.txt", "r", encoding="utf-8") as f:
        urls = [l.strip() for l in f if l.strip() and l.startswith("http")]

    if not urls:
        print("âŒ config.txt ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„URL")
        return

    print("=" * 60)
    print("EPG æºæŠ“å–ç»Ÿè®¡ï¼ˆå¤±è´¥è‡ªåŠ¨é‡è¯•ï¼‰")
    print("=" * 60)

    with ThreadPoolExecutor(max_workers=6) as executor:
        future_map = {executor.submit(fetch_with_retry, u): u for u in urls}
        for fut in future_map:
            u = future_map[fut]
            ok, tree, ch, pg, retry_cnt = fut.result()
            if ok:
                success_cnt += 1
                total_ch += ch
                total_pg += pg
                log_retry = f"[é‡è¯•{retry_cnt-1}æ¬¡]" if retry_cnt > 1 else ""
                print(f"âœ… {u[:55]}... {log_retry}æˆåŠŸ | é¢‘é“ {ch:>4} | èŠ‚ç›® {pg:>6}")
                for node in tree:
                    if node.tag == "channel":
                        all_channels.append(node)
                    elif node.tag == "programme":
                        all_programs.append(node)
            else:
                fail_cnt += 1

    if fail_cnt > 0:
        print(f"âŒ å…± {fail_cnt} ä¸ªæºç»{MAX_RETRY}æ¬¡é‡è¯•åä»å¤±è´¥ï¼Œå·²è·³è¿‡")

    print("=" * 60)
    print(f"æ±‡æ€»ï¼šæˆåŠŸ {success_cnt} ä¸ª | å¤±è´¥ {fail_cnt} ä¸ª | æ€»é¢‘é“ {total_ch} | æ€»èŠ‚ç›® {total_pg}")
    print("=" * 60)

    try:
        with gzip.open(weifang_gz_file, "rb") as f:
            wf_content = f.read().decode("utf-8")
            parser = etree.XMLParser(recover=True)
            wf_tree = etree.fromstring(wf_content.encode("utf-8"), parser=parser)
            wf_ch = len(wf_tree.xpath("//channel"))
            wf_pg = len(wf_tree.xpath("//programme"))

        if wf_ch > 0 and wf_pg > 0:
            print(f"ğŸ“º æ½åŠæœ¬åœ°æºï¼šé¢‘é“ {wf_ch} | èŠ‚ç›® {wf_pg}ï¼ˆæœ¬å‘¨ä¸€~å‘¨æ—¥å®Œæ•´7å¤©ï¼‰")
            for node in wf_tree:
                if node.tag == "channel":
                    all_channels.append(node)
                elif node.tag == "programme":
                    all_programs.append(node)
        else:
            print("âš ï¸ æ½åŠæœ¬åœ°æºæŠ“å–å¤±è´¥ï¼Œå·²è·³è¿‡")
    except Exception as e:
        print(f"âš ï¸ æ½åŠæœ¬åœ°æºè¯»å–å¤±è´¥: {e}")

    print(f"å¤„ç†å‰: é¢‘é“ {len(all_channels)} ä¸ª, èŠ‚ç›® {len(all_programs)} ä¸ª")

    # ====================== ä¿®å¤é¢‘é“å»é‡ ======================
    seen_channel_names = set()
    unique_channels = []
    channel_id_mapping = {}  # åŸå§‹ID -> ä¿ç•™ID
    name_to_id = {}  # é¢‘é“åç§° -> é¢‘é“ID
    
    for ch in all_channels:
        try:
            # è·å–é¢‘é“ID
            channel_id = ch.get('id', '')
            if not channel_id:
                continue
                
            # è·å–é¢‘é“åç§°
            display_name = ch.findtext("display-name", "").strip()
            if not display_name:
                display_name = channel_id
            
            # æ ‡å‡†åŒ–åç§°ï¼ˆå°å†™ï¼‰
            normalized_name = display_name.lower()
            
            if normalized_name not in seen_channel_names:
                seen_channel_names.add(normalized_name)
                unique_channels.append(ch)
                name_to_id[normalized_name] = channel_id
                # è‡ªèº«æ˜ å°„
                channel_id_mapping[channel_id] = channel_id
            else:
                # é‡å¤é¢‘é“ï¼Œæ˜ å°„åˆ°å·²å­˜åœ¨çš„é¢‘é“ID
                existing_id = name_to_id.get(normalized_name)
                if existing_id:
                    channel_id_mapping[channel_id] = existing_id
        except Exception as e:
            print(f"âš ï¸ å¤„ç†é¢‘é“æ—¶å‡ºé”™: {e}")
            continue
    
    print(f"é¢‘é“å»é‡å: {len(unique_channels)} ä¸ªå”¯ä¸€é¢‘é“")
    
    # ====================== ä¿®å¤èŠ‚ç›®å¤„ç† ======================
    valid_programs = []
    program_keys = set()  # ç”¨äºå»é‡
    
    for prog in all_programs:
        try:
            old_channel_id = prog.get('channel', '')
            start = prog.get('start', '')
            stop = prog.get('stop', '')
            title_elem = prog.find("title")
            
            # éªŒè¯å¿…è¦å­—æ®µ
            if not all([old_channel_id, start, stop]):
                continue
                
            if title_elem is None or not title_elem.text:
                continue
                
            title = title_elem.text.strip()
            if len(title) < 2:
                continue
            
            # æŸ¥æ‰¾æ­£ç¡®çš„é¢‘é“ID
            new_channel_id = channel_id_mapping.get(old_channel_id, old_channel_id)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„é¢‘é“å­˜åœ¨
            channel_exists = any(ch.get('id') == new_channel_id for ch in unique_channels)
            if not channel_exists:
                # å°è¯•é€šè¿‡åç§°æŸ¥æ‰¾
                for ch in unique_channels:
                    ch_name = ch.findtext("display-name", "").strip().lower()
                    if old_channel_id.lower() in ch_name or ch_name in old_channel_id.lower():
                        new_channel_id = ch.get('id', '')
                        break
            
            if not new_channel_id:
                continue
            
            # åˆ›å»ºå»é‡é”®
            program_key = f"{new_channel_id}|{start}|{title}"
            
            if program_key not in program_keys:
                program_keys.add(program_key)
                
                # åˆ›å»ºæ–°çš„èŠ‚ç›®å…ƒç´ 
                new_prog = etree.Element("programme", 
                                        channel=new_channel_id,
                                        start=start,
                                        stop=stop)
                title_elem = etree.SubElement(new_prog, "title", lang="zh")
                title_elem.text = title
                
                valid_programs.append(new_prog)
                
        except Exception as e:
            print(f"âš ï¸ å¤„ç†èŠ‚ç›®æ—¶å‡ºé”™: {e}")
            continue
    
    print(f"èŠ‚ç›®å»é‡å: {len(valid_programs)} ä¸ªæœ‰æ•ˆèŠ‚ç›®")
    print(f"ğŸ¯ å»é‡ç‡: {(len(all_programs) - len(valid_programs)) / len(all_programs) * 100:.1f}%")
    
    # æŒ‰é¢‘é“å’Œå¼€å§‹æ—¶é—´æ’åº
    valid_programs.sort(key=lambda x: (x.get('channel', ''), x.get('start', '')))
    
    # ç”Ÿæˆæœ€ç»ˆXML
    final_root = etree.Element("tv")
    
    # æ·»åŠ é¢‘é“
    for ch in unique_channels:
        final_root.append(ch)
    
    # æ·»åŠ èŠ‚ç›®
    for prog in valid_programs:
        final_root.append(prog)
    
    # ç”ŸæˆXMLå­—ç¬¦ä¸²
    xml_str = etree.tostring(final_root, encoding="utf-8", pretty_print=True, xml_declaration=True)
    
    # éªŒè¯XMLæ ¼å¼
    try:
        parser = etree.XMLParser(recover=True)
        test_tree = etree.fromstring(xml_str, parser=parser)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ ¼å¼é—®é¢˜
        test_channels = test_tree.xpath("//channel")
        test_programs = test_tree.xpath("//programme")
        
        print(f"âœ… XMLéªŒè¯é€šè¿‡: {len(test_channels)} é¢‘é“, {len(test_programs)} èŠ‚ç›®")
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆçš„XMLæ ¼å¼é”™è¯¯: {e}")
        # åˆ›å»ºæœ€å°å¯ç”¨çš„XML
        final_root = etree.Element("tv")
        xml_str = etree.tostring(final_root, encoding="utf-8", pretty_print=True, xml_declaration=True)
    
    # ä¿å­˜æ–‡ä»¶
    output_path = os.path.join(OUTPUT_DIR, "epg.gz")
    with gzip.open(output_path, "wb") as f:
        f.write(xml_str)
    
    # è®¡ç®—æ–‡ä»¶å¤§å°
    file_size_mb = os.path.getsize(output_path) / 1024 / 1024
    print(f"âœ… æœ€ç»ˆè¾“å‡ºï¼šé¢‘é“ {len(unique_channels)} ä¸ª | èŠ‚ç›® {len(valid_programs)} ä¸ª")
    print(f"ğŸ“¦ æ–‡ä»¶å¤§å°ï¼š{file_size_mb:.2f} MB")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶ï¼š{output_path}")
    print("=" * 60)

# ====================== å…¥å£ ======================
if __name__ == "__main__":
    try:
        print("å¼€å§‹æŠ“å–æ½åŠæœ¬åœ°EPG...")
        wf_gz = crawl_weifang()
        print("å¼€å§‹åˆå¹¶æ‰€æœ‰EPGæº...")
        merge_all(wf_gz)
        print("âœ… EPGåˆå¹¶å®Œæˆï¼")
    except Exception as e:
        print(f"âŒ è„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
