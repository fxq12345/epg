import os
import gzip
import re
import time
import signal
import logging
from typing import List, Dict, Set, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
import requests
from lxml import etree
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ===================== åŸç‰ˆé…ç½®å®Œå…¨ä¸åŠ¨ =====================
CONFIG_FILE = "config.txt"
OUTPUT_DIR = "output"
LOG_FILE = "epg_merge.log"
MAX_WORKERS = 5
TIMEOUT = 30
CORE_RETRY_COUNT = 2

LOCAL_WEIFANG_EPG = os.path.join(OUTPUT_DIR, "weifang.xml")

# 10åˆ†é’Ÿè¶…æ—¶å¼ºåˆ¶é€€å‡º
GLOBAL_TIMEOUT_SECONDS = 600
def timeout_handler(signum, frame):
    os._exit(0)
signal.signal(signal.SIGALRM, timeout_handler)
signal.alarm(GLOBAL_TIMEOUT_SECONDS)

# æ—¥å¿—å®Œå…¨ä¿ç•™ä½ åŸç‰ˆ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# ===================== ä½ åŸç‰ˆä»£ç å…¨éƒ¨ä¸åŠ¨ =====================
class EPGGenerator:
    def __init__(self):
        self.session = self._create_session()
        self.channel_ids: Set[str] = set()
        self.all_channels: List = []
        self.all_programs: List = []
        self.channel_programs: Dict[str, List] = {}
        os.makedirs(OUTPUT_DIR, exist_ok=True)

    def _create_session(self) -> requests.Session:
        session = requests.Session()
        retry_strategy = Retry(
            total=CORE_RETRY_COUNT + 2,
            backoff_factor=1.5,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "application/xml, */*",
            "Accept-Encoding": "gzip, deflate"
        })
        return session

    def read_epg_sources(self) -> List[str]:
        if not os.path.exists(CONFIG_FILE):
            logging.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {CONFIG_FILE}")
            return []
        try:
            with open(CONFIG_FILE, "r", encoding="utf-8") as f:
                sources = [
                    line.strip() for line_num, line in enumerate(f, 1)
                    if line.strip() and not line.startswith("#") and line.startswith(("http://", "https://"))
                ]
            logging.info(f"ä»{CONFIG_FILE}è¯»å–åˆ°{len(sources)}æ¡EPGæº:")
            for idx, source in enumerate(sources, 1):
                logging.info(f"  {idx}. {source[:60]}...")
            return sources
        except Exception as e:
            logging.error(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
            return []

    def clean_xml_content(self, content: str) -> str:
        content_clean = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', content)
        return content_clean.replace('& ', '&amp; ')

    def fetch_single_source(self, source: str) -> Tuple[bool, any]:
        try:
            start_time = time.time()
            response = self.session.get(source, timeout=TIMEOUT)
            response.raise_for_status()
            if source.endswith('.gz'):
                content = gzip.decompress(response.content).decode('utf-8')
            else:
                content = response.text
            content_clean = self.clean_xml_content(content)
            xml_tree = etree.fromstring(content_clean.encode('utf-8'))
            cost_time = time.time() - start_time
            logging.info(f"âœ… æŠ“å–æˆåŠŸ: {source[:30]}... (è€—æ—¶{cost_time:.2f}s)")
            return True, xml_tree
        except Exception as e:
            logging.warning(f"âš ï¸ æŠ“å–å¤±è´¥(è·³è¿‡): {source[:30]}... -> {str(e)[:50]}")
            return False, None

    def process_channels_and_programs(self, xml_tree, source: str):
        channel_count = 0
        for channel in xml_tree.xpath("//channel"):
            channel_id = channel.get("id", "").strip()
            if not channel_id or channel_id in self.channel_ids:
                continue
            self.channel_ids.add(channel_id)
            self.all_channels.append(channel)
            self.channel_programs[channel_id] = []
            channel_count += 1
        program_count = 0
        for program in xml_tree.xpath("//programme"):
            channel_id = program.get("channel", "").strip()
            if channel_id and channel_id in self.channel_programs:
                self.channel_programs[channel_id].append(program)
                self.all_programs.append(program)
                program_count += 1
        logging.info(f"ğŸ”§ å¤„ç†{source[:30]}...: æ–°å¢é¢‘é“{channel_count}ä¸ªï¼Œæ–°å¢èŠ‚ç›®{program_count}ä¸ª")

    def process_local_weifang_epg(self):
        if not os.path.exists(LOCAL_WEIFANG_EPG):
            logging.warning(f"âš ï¸ æœ¬åœ°æ½åŠEPGæ–‡ä»¶ä¸å­˜åœ¨: {LOCAL_WEIFANG_EPG}ï¼Œè·³è¿‡")
            return
        try:
            logging.info(f"å¼€å§‹åˆå¹¶æœ¬åœ°æ½åŠEPGæ–‡ä»¶")
            with open(LOCAL_WEIFANG_EPG, "r", encoding="utf-8") as f:
                content = f.read()
            content_clean = self.clean_xml_content(content)
            xml_tree = etree.fromstring(content_clean.encode('utf-8'))
            self.process_channels_and_programs(xml_tree, "æœ¬åœ°æ½åŠEPG")
            logging.info(f"âœ… æˆåŠŸåˆå¹¶æœ¬åœ°æ½åŠEPG")
        except Exception as e:
            logging.warning(f"âš ï¸ åˆå¹¶æœ¬åœ°æ½åŠEPGå¤±è´¥ï¼Œå·²è·³è¿‡: {str(e)}")

    def fetch_and_process_all_sources(self, sources: List[str]):
        logging.info("\nå¼€å§‹æŠ“å–æ‰€æœ‰EPGæº:")
        with ThreadPoolExecutor(max_workers=min(MAX_WORKERS, len(sources))) as executor:
            future_to_source = {executor.submit(self.fetch_single_source, source): source for source in sources}
            for future in as_completed(future_to_source):
                source = future_to_source[future]
                try:
                    success, xml_tree = future.result()
                    if success and xml_tree is not None:
                        self.process_channels_and_programs(xml_tree, source)
                except Exception as e:
                    logging.warning(f"å¤„ç†æº{source[:30]}...å¤±è´¥: {str(e)}")
        self.process_local_weifang_epg()

    def generate_final_xml(self) -> str:
        xml_declare = '<?xml version="1.0" encoding="UTF-8"?>'
        root = etree.fromstring(f"{xml_declare}<tv></tv>".encode("utf-8"))
        for channel in self.all_channels:
            root.append(channel)
        for program in self.all_programs:
            root.append(program)
        return etree.tostring(root, encoding="utf-8", pretty_print=True).decode("utf-8")

    def save_files(self, xml_content: str):
        xml_path = os.path.join(OUTPUT_DIR, "epg.xml")
        with open(xml_path, "w", encoding="utf-8") as f:
            f.write(xml_content)
        gz_path = os.path.join(OUTPUT_DIR, "epg.gz")
        with gzip.open(gz_path, "wb") as f:
            f.write(xml_content.encode("utf-8"))
        logging.info(f"\nğŸ’¾ æ–‡ä»¶ä¿å­˜æˆåŠŸ:")
        logging.info(f"  - XMLæ–‡ä»¶: {os.path.abspath(xml_path)}")
        logging.info(f"  - GZIPæ–‡ä»¶: {os.path.abspath(gz_path)}")

    def print_statistics(self):
        logging.info("\n" + "="*50)
        logging.info("ğŸ“Š EPGåˆå¹¶ç»Ÿè®¡æŠ¥å‘Š")
        logging.info(f"  æ€»é¢‘é“æ•°: {len(self.channel_ids)}")
        logging.info(f"  æ€»èŠ‚ç›®æ•°: {len(self.all_programs)}")
        logging.info("="*50)

    def run(self):
        start_time = time.time()
        logging.info("\n" + "="*50)
        logging.info("ğŸš€ å¯åŠ¨EPGåˆå¹¶æµç¨‹")
        logging.info("="*50)
        try:
            sources = self.read_epg_sources()
            if not sources:
                logging.warning("âŒ æ— å¯ç”¨EPGæºï¼Œç»§ç»­æœ¬åœ°æº")
            self.fetch_and_process_all_sources(sources)
            xml_content = self.generate_final_xml()
            self.save_files(xml_content)
            self.print_statistics()
            total_time = time.time() - start_time
            logging.info(f"\nâœ… åˆå¹¶æµç¨‹å®Œæˆ! æ€»è€—æ—¶: {total_time:.2f}ç§’")
            return True
        except Exception as e:
            logging.warning(f"\nâš ï¸ åˆå¹¶æµç¨‹å¼‚å¸¸ï¼Œå·²è·³è¿‡: {str(e)}")
            return False

# ===================== åµŒå…¥æ½åŠæŠ“å–ï¼ˆä½ åŸç‰ˆé€»è¾‘å®Œå…¨ä¸å˜ï¼‰ =====================
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
from xml.dom import minidom
from datetime import timedelta
import random

WEIFANG_CHANNELS = [
    ("æ½åŠæ–°é—»é¢‘é“", "https://m.tvsou.com/epg/db502561"),
    ("æ½åŠç»æµç”Ÿæ´»é¢‘é“", "https://m.tvsou.com/epg/47a9d24a"),
    ("æ½åŠç§‘æ•™é¢‘é“", "https://m.tvsou.com/epg/d131d3d1"),
    ("æ½åŠå…¬å…±é¢‘é“", "https://m.tvsou.com/epg/c06f0cc0")
]

WEEK_MAP = {
    "å‘¨ä¸€": "w1", "å‘¨äºŒ": "w2", "å‘¨ä¸‰": "w3", "å‘¨å››": "w4",
    "å‘¨äº”": "w5", "å‘¨å…­": "w6", "å‘¨æ—¥": "w7"
}

HEADERS_WF = {
    "User-Agent": "Mozilla/5.0 (Linux; Android 12; Mobile) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Mobile Safari/537.36",
    "Referer": "https://www.bing.com"
}

def time_to_xmltv(base_date, time_str):
    try:
        hh, mm = time_str.strip().split(":")
        dt = datetime.combine(base_date, datetime.min.time().replace(hour=int(hh), minute=int(mm)))
        return dt.strftime("%Y%m%d%H%M%S +0800")
    except:
        return ""

def get_page_html(url):
    try:
        resp = requests.get(url, headers=HEADERS_WF, timeout=15)
        resp.encoding = 'utf-8'
        if "èŠ‚ç›®å•" in resp.text or len(re.findall(r'\d{1,2}:\d{2}', resp.text)) > 5:
            return resp.text
    except:
        return ""
    return ""

def get_day_program(channel_name, base_url, week_name, w_suffix):
    url = f"{base_url}/{w_suffix}" if not base_url.endswith('/') else f"{base_url}{w_suffix}"
    programs = []
    try:
        html = get_page_html(url)
        if not html:
            return programs
        soup = BeautifulSoup(html, "html.parser")
        items = soup.find_all("div", class_=re.compile("program-item|time-item", re.I)) or soup.find_all("li")
        for item in items:
            match = re.search(r'(\d{1,2}:\d{2})\s*(.+)', item.get_text(strip=True))
            if match:
                t, title = match.groups()
                if len(title) > 1 and 'å¹¿å‘Š' not in title:
                    programs.append((t.strip(), title.strip()))
        programs = sorted(list(set(programs)), key=lambda x: x[0])
    except:
        pass
    return programs

def build_weifang_xml(channel_data):
    root = ET.Element("tv")
    root.set("source-info-name", "Weifang Local EPG")
    for ch_name, _ in WEIFANG_CHANNELS:
        ch = ET.SubElement(root, "channel", id=ch_name)
        ET.SubElement(ch, "display-name", lang="zh").text = ch_name
    today = datetime.now()
    monday = today - timedelta(days=today.weekday())
    for ch_name, week_list in channel_data.items():
        for i, (wname, wsuffix, progs) in enumerate(week_list):
            current_date = monday + timedelta(days=i)
            for idx in range(len(progs)):
                s_time, title = progs[idx]
                e_time = progs[idx+1][0] if idx < len(progs)-1 else (datetime.strptime(s_time,"%H:%M")+timedelta(minutes=30)).strftime("%H:%M")
                s_xml = time_to_xmltv(current_date, s_time)
                e_xml = time_to_xmltv(current_date, e_time)
                if s_xml and e_xml:
                    prog = ET.SubElement(root, "programme")
                    prog.set("start", s_xml)
                    prog.set("stop", e_xml)
                    prog.set("channel", ch_name)
                    ET.SubElement(prog, "title", lang="zh").text = title
    rough_str = ET.tostring(root, encoding="utf-8")
    return minidom.parseString(rough_str).toprettyxml(indent="  ", encoding="utf-8")

def run_weifang_crawler():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    out_path = os.path.join(OUTPUT_DIR, "weifang.xml")
    try:
        channel_data = {}
        for ch_name, base_url in WEIFANG_CHANNELS:
            week_data = []
            for wname, wsuffix in WEEK_MAP.items():
                progs = get_day_program(ch_name, base_url, wname, wsuffix)
                week_data.append((wname, wsuffix, progs))
                time.sleep(0.7)
            channel_data[ch_name] = week_data
        xml_bytes = build_weifang_xml(channel_data)
        with open(out_path, "wb") as f:
            f.write(xml_bytes)
    except Exception as e:
        with open(out_path, "w", encoding="utf-8") as f:
            f.write('<?xml version="1.0" encoding="utf-8"?>\n<tv></tv>')

# ===================== ä¸»å…¥å£ï¼ˆåŸç‰ˆç»“æ„ä¸å˜ï¼‰ =====================
def main():
    try:
        run_weifang_crawler()
    except:
        pass
    EPGGenerator().run()

if __name__ == "__main__":
    main()
