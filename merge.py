import os
import gzip
import re
import time
import signal
import requests
from lxml import etree
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor

# 10åˆ†é’Ÿå¼ºåˆ¶ç»ˆæ­¢
signal.signal(signal.SIGALRM, lambda s, f: os._exit(0))
signal.alarm(600)

OUTPUT_DIR = "output"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ====================== æ½åŠ4é¢‘é“é…ç½® ======================
WEIFANG_CHANNELS = [
    ("æ½åŠæ–°é—»é¢‘é“", "https://m.tvsou.com/epg/db502561"),
    ("æ½åŠç»æµç”Ÿæ´»é¢‘é“", "https://m.tvsou.com/epg/47a9d24a"),
    ("æ½åŠç§‘æ•™é¢‘é“", "https://m.tvsou.com/epg/d131d3d1"),
    ("æ½åŠå…¬å…±é¢‘é“", "https://m.tvsou.com/epg/c06f0cc0")
]

WEEK_MAP = {
    "å‘¨ä¸€": "w1", "å‘¨äºŒ": "w2", "å‘¨ä¸‰": "w3", "å‘¨å››": "w4",
    "å‘¨äº”": "w5", "å‘¨å…­": "w6", "å‘¨æ—¥": "w7"
}

MAX_RETRY = 2

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Linux; Android 12; Mobile) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Mobile Safari/537.36",
    "Referer": "https://www.bing.com"
}

SELENIUM_AVAILABLE = False

# ====================== å·¥å…·å‡½æ•° ======================
def time_to_xmltv(base_date, time_str):
    try:
        hh, mm = time_str.strip().split(":")
        dt = datetime.combine(base_date, datetime.min.time().replace(hour=int(hh), minute=int(mm)))
        return dt.strftime("%Y%m%d%H%M%S +0800")
    except:
        return ""

def get_page_html(url):
    try:
        resp = requests.get(url, headers=HEADERS, timeout=12)
        resp.encoding = 'utf-8'
        if re.findall(r'\d{1,2}:\d{2}', resp.text):
            return resp.text
    except:
        pass
    return ""

# ====================== æŠ“å–æ½åŠ7å¤© ======================
def get_channel_7days(channel_name, base_url):
    week_list = list(WEEK_MAP.items())
    today = datetime.now()
    monday = today - timedelta(days=today.weekday())
    channel_progs = []

    for i, (week_name, w_suffix) in enumerate(week_list):
        current_date = monday + timedelta(days=i)
        url = f"{base_url}/{w_suffix}" if not base_url.endswith('/') else f"{base_url}{w_suffix}"
        html = get_page_html(url)
        if not html:
            time.sleep(1)
            continue

        soup = BeautifulSoup(html, "html.parser")
        items = soup.find_all("div", class_=re.compile("program-item|time-item", re.I)) or soup.find_all("li")
        day_progs = []
        for item in items:
            txt = item.get_text(strip=True)
            match = re.search(r'(\d{1,2}:\d{2})\s*(.+)', txt)
            if not match:
                continue
            t_str, title = match.groups()
            if len(title) < 2 or 'å¹¿å‘Š' in title or 'æŠ¥æ—¶' in title:
                continue
            day_progs.append((t_str.strip(), title.strip()))

        day_progs = sorted(list(set(day_progs)), key=lambda x: x[0])
        for idx in range(len(day_progs)):
            t_start, title = day_progs[idx]
            if idx < len(day_progs)-1:
                t_end = day_progs[idx+1][0]
            else:
                h, m = map(int, t_start.split(':'))
                t_end = (datetime(2000,1,1,h,m)+timedelta(minutes=30)).strftime("%H:%M")

            start = time_to_xmltv(current_date, t_start)
            end = time_to_xmltv(current_date, t_end)
            if start and end:
                channel_progs.append((start, end, title))
        time.sleep(1)
    return channel_progs

def crawl_weifang():
    try:
        root = etree.Element("tv")
        for ch_name, _ in WEIFANG_CHANNELS:
            ch = etree.SubElement(root, "channel", id=ch_name)
            dn = etree.SubElement(ch, "display-name", lang="zh")
            dn.text = ch_name

        for ch_name, base_url in WEIFANG_CHANNELS:
            progs = get_channel_7days(ch_name, base_url)
            for s, e, t in progs:
                p = etree.SubElement(root, "programme", start=s, stop=e, channel=ch_name)
                te = etree.SubElement(p, "title", lang="zh")
                te.text = t

        wf_path = os.path.join(OUTPUT_DIR, "weifang.gz")
        xml_content = etree.tostring(root, encoding="utf-8", xml_declaration=True)
        with gzip.open(wf_path, "wb") as f:
            f.write(xml_content)
        print(f"âœ… æ½åŠEPGå·²ä¿å­˜")
        return wf_path
    except Exception as e:
        print(f"âŒ æ½åŠæŠ“å–å¤±è´¥: {e}")
        empty = b'<?xml version="1.0" encoding="utf-8"?>\n<tv></tv>'
        p = os.path.join(OUTPUT_DIR, "weifang.gz")
        with gzip.open(p, "wb") as f:
            f.write(empty)
        return p

# ====================== æŠ“å–ä¸Šæ¸¸æº ======================
def fetch_with_retry(u):
    for _ in range(MAX_RETRY):
        try:
            r = requests.get(u, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
            if r.status_code not in (200, 206):
                time.sleep(1)
                continue
            c = gzip.decompress(r.content).decode("utf-8","ignore") if u.endswith(".gz") else r.text
            c = re.sub(r'[\x00-\x1f]', '', c).replace("& ", "&amp; ")
            tree = etree.fromstring(c.encode("utf-8"))
            ch = len(tree.xpath("//channel"))
            pg = len(tree.xpath("//programme"))
            if ch>0 and pg>0:
                return True, tree
        except:
            time.sleep(1)
    return False, None

def merge_all(weifang_gz):
    if not os.path.exists("config.txt"):
        print("âŒ æ— config.txt")
        return

    urls = [l.strip() for l in open("config.txt","utf-8") if l.strip().startswith("http")]
    if not urls:
        print("âŒ æ— æœ‰æ•ˆURL")
        return

    print("å¼€å§‹æŠ“å–æ‰€æœ‰æº...")
    all_trees = []
    with ThreadPoolExecutor(max_workers=6) as exec:
        res = {exec.submit(fetch_with_retry, u):u for u in urls}
        for f in res:
            ok, t = f.result()
            if ok:
                all_trees.append(t)

    # è¯»å–æ½åŠ
    try:
        with gzip.open(weifang_gz,"rb") as f:
            wf_tree = etree.fromstring(f.read())
            all_trees.append(wf_tree)
    except:
        pass

    # ====================== è¶…çº§è½»é‡å»é‡ï¼ˆå…¼å®¹æ‰€æœ‰æ’­æ”¾å™¨ï¼‰ ======================
    final = etree.Element("tv")
    seen_channel_id = set()
    seen_program_key = set()

    for tree in all_trees:
        for node in tree:
            if node.tag == "channel":
                cid = node.get("id","")
                if cid and cid not in seen_channel_id:
                    seen_channel_id.add(cid)
                    final.append(node)

            elif node.tag == "programme":
                c = node.get("channel","")
                s = node.get("start","")
                e = node.get("stop","")
                key = (c, s, e)
                if c and s and e and key not in seen_program_key:
                    seen_program_key.add(key)
                    final.append(node)

    # è¾“å‡º
    out = os.path.join(OUTPUT_DIR, "epg.gz")
    xml = etree.tostring(final, encoding="utf-8", xml_declaration=True)
    with gzip.open(out,"wb") as f:
        f.write(xml)

    size_mb = os.path.getsize(out)/1024/1024
    print(f"âœ… åˆå¹¶å®Œæˆï¼æ–‡ä»¶å¤§å°ï¼š{size_mb:.2f}MB")
    print(f"âœ… é¢‘é“ï¼š{len(seen_channel_id)}  èŠ‚ç›®ï¼š{len(seen_program_key)}")
    print("ğŸ“ è¾“å‡ºï¼š" + out)

# ====================== å…¥å£ ======================
if __name__ == "__main__":
    try:
        wf = crawl_weifang()
        merge_all(wf)
        print("ğŸ‰ å…¨éƒ¨å®Œæˆï¼Œæ‰€æœ‰æ’­æ”¾å™¨é€šç”¨ï¼")
    except Exception as e:
        print(f"âŒ å¤±è´¥ï¼š{e}")
