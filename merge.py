import os
import gzip
import requests
import time
from datetime import datetime
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
from lxml import etree
import requests.adapters
from requests.packages.urllib3.util.retry import Retry

# å…¨å±€é…ç½®
OUTPUT_DIR = "output"
MAX_RETRY = 3
TIMEOUT = 30

def create_session():
    session = requests.Session()
    retry_strategy = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET"]
    )
    adapter = requests.adapters.HTTPAdapter(
        max_retries=retry_strategy,
        pool_connections=10,
        pool_maxsize=10
    )
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    return session

def fetch_with_retry(url):
    session = create_session()
    retry_cnt = 0
    while retry_cnt < MAX_RETRY:
        retry_cnt += 1
        try:
            print(f"ğŸ”„ æŠ“å–: {url[:60]}... ç¬¬{retry_cnt}æ¬¡")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            resp = session.get(url, timeout=TIMEOUT, headers=headers, stream=True)
            resp.raise_for_status()
            content = resp.content
            
            try_gzip = False
            if url.endswith(".gz"):
                try_gzip = True
            elif resp.headers.get("content-encoding") == "gzip":
                try_gzip = True
            elif resp.headers.get("Content-Type", "").endswith("gzip"):
                try_gzip = True
            
            if try_gzip:
                try:
                    content = gzip.decompress(content)
                    print(f"  æ£€æµ‹åˆ°gzipæ ¼å¼ï¼Œå·²è§£å‹")
                except (gzip.BadGzipFile, OSError):
                    print(f"  è­¦å‘Šï¼šæ ‡è®°ä¸ºgzipä½†å®é™…ä¸æ˜¯ï¼ŒæŒ‰æ™®é€šXMLå¤„ç†")
            
            try:
                xml_str = content.decode('utf-8')
            except UnicodeDecodeError:
                try:
                    xml_str = content.decode('gbk')
                except:
                    xml_str = content.decode('utf-8', errors='ignore')
            
            tree = etree.fromstring(xml_str.encode('utf-8'))
            ch = len(tree.findall(".//channel"))
            pg = len(tree.findall(".//programme"))
            print(f"âœ… æˆåŠŸ: é¢‘é“ {ch} èŠ‚ç›® {pg}")
            return True, tree, ch, pg
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ ç½‘ç»œé”™è¯¯: {type(e).__name__}: {str(e)[:80]}")
        except etree.XMLSyntaxError as e:
            print(f"âŒ XMLè§£æé”™è¯¯: {str(e)[:80]}")
        except Exception as e:
            print(f"âŒ å…¶ä»–é”™è¯¯: {type(e).__name__}: {str(e)[:80]}")
        
        if retry_cnt < MAX_RETRY:
            time.sleep(2 ** retry_cnt)
    
    return False, None, 0, 0

def merge_all(local_file):
    all_channels = []
    all_programs = []

    # è¯»å–å¹¶å»é‡URL
    with open("config.txt", "r", encoding="utf-8") as f:
        urls = list({line.strip() for line in f if line.strip().startswith("http")})

    print(f"ğŸ“¥ ç½‘ç»œæºå…± {len(urls)} ä¸ª")

    xml_trees = []
    with ThreadPoolExecutor(max_workers=5) as executor:
        tasks = {executor.submit(fetch_with_retry, u): u for u in urls}
        for t in tasks:
            ok, tree, ch, pg = t.result()
            if ok and tree is not None:
                xml_trees.append(tree)

    print(f"ğŸ“¥ æˆåŠŸåŠ è½½ {len(xml_trees)} ä¸ªXML")

    # ç»Ÿä¸€é¢‘é“IDä¸ºåç§°ï¼ˆæŒ‰é¢‘é“åå»é‡ï¼‰
    id_map = {}
    for tree in xml_trees:
        for ch in tree.findall(".//channel"):
            cid = ch.get("id", "").strip()
            dn = ch.find("display-name")
            name = dn.text.strip() if (dn is not None and dn.text) else cid
            if cid and name and cid not in id_map:
                id_map[cid] = name

    exist_names = set()
    for tree in xml_trees:
        for ch in tree.findall(".//channel"):
            old_id = ch.get("id", "").strip()
            name = id_map.get(old_id, old_id)
            if name and name not in exist_names:
                exist_names.add(name)
                ch.set("id", name)
                all_channels.append(ch)

    for tree in xml_trees:
        for p in tree.findall(".//programme"):
            old_c = p.get("channel", "").strip()
            new_c = id_map.get(old_c, old_c)
            if new_c:
                p.set("channel", new_c)
            tit = p.find("title")
            if tit is None or not tit.text or len(tit.text.strip()) < 1:
                continue
            all_programs.append(p)

    # åˆå¹¶æ½åŠæœ¬åœ°æº
    if os.path.exists(local_file):
        try:
            with gzip.open(local_file, "rb") as f:
                local_tree = etree.fromstring(f.read())
            local_map = {}
            for ch in local_tree.findall(".//channel"):
                cid = ch.get("id", "").strip()
                dn = ch.find("display-name")
                name = dn.text.strip() if (dn is not None and dn.text) else cid
                local_map[cid] = name
                if name and name not in exist_names:
                    exist_names.add(name)
                    ch.set("id", name)
                    all_channels.append(ch)
            for p in local_tree.findall(".//programme"):
                old_c = p.get("channel", "").strip()
                new_c = local_map.get(old_c, old_c)
                if new_c:
                    p.set("channel", new_c)
                tit = p.find("title")
                if tit is None or not tit.text or len(tit.text.strip()) < 1:
                    continue
                all_programs.append(p)
            print("âœ… æ½åŠæœ¬åœ°4é¢‘é“å·²åˆå¹¶")
        except Exception as e:
            print(f"âš ï¸ æ½åŠæºè¯»å–å¤±è´¥ï¼Œå·²è·³è¿‡: {e}")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°æ½åŠæºæ–‡ä»¶ {local_file}ï¼Œå·²è·³è¿‡")

    # é¢‘é“æ’åºï¼šå±±ä¸œ > æ½åŠ > CCTV > å«è§† > å…¶ä»–
    def channel_sort_key(channel_elem):
        name = channel_elem.get("id", "").strip()
        if "å±±ä¸œ" in name:
            return 0, name
        elif "æ½åŠ" in name:
            return 1, name
        elif "CCTV" in name:
            return 2, name
        elif "å«è§†" in name:
            return 3, name
        else:
            return 99, name

    all_channels.sort(key=channel_sort_key)

    # ====================== çº¯å»é‡ï¼Œä¸åˆ ä»»ä½•æ—¥æœŸ ======================
    print(f"åŸå§‹èŠ‚ç›®æ•°: {len(all_programs)}")
    unique = []
    seen = set()

    for p in all_programs:
        try:
            key = p.get("channel") + "|" + p.get("start")
            if key in seen:
                continue

            title_elem = p.find("title")
            title = title_elem.text.strip() if (title_elem is not None and title_elem.text) else ""
            if not title:
                continue

            seen.add(key)
            unique.append(p)
        except:
            continue

    unique.sort(key=lambda x: (x.get("channel"), x.get("start")))
    print(f"å»é‡åèŠ‚ç›®: {len(unique)}")

    # ====================== è‡ªåŠ¨ç»Ÿè®¡å±±ä¸œé¢‘é“èŠ‚ç›® ======================
    print("\n" + "="*60)
    print("ğŸ“º å±±ä¸œé‡ç‚¹é¢‘é“èŠ‚ç›®ç»Ÿè®¡ï¼ˆä¸Šæ¸¸æºæœ‰å°±æ˜¾ç¤ºï¼Œæ— åˆ™ä¸º0ï¼‰")
    print("="*60)

    target_names = [
        "å±±ä¸œé½é²",
        "å±±ä¸œç»¼è‰º",
        "å±±ä¸œç”Ÿæ´»",
        "å±±ä¸œå°‘å„¿",
        "å±±ä¸œä½“è‚²",
        "å±±ä¸œæ–°é—»",
        "å±±ä¸œæ–‡æ—…"
    ]

    chan_days = defaultdict(set)
    for p in unique:
        c = p.get("channel", "")
        s = p.get("start", "")
        if len(s) >= 8:
            day = s[:8]
            for tn in target_names:
                if tn in c:
                    chan_days[tn].add(day)
                    break

    for tn in target_names:
        days = sorted(chan_days.get(tn, set()))
        if days:
            print(f"âœ… {tn}ï¼šæœ‰ {len(days)} å¤© | {days[0][:4]}-{days[0][4:6]}-{days[0][6:]} ~ {days[-1][:4]}-{days[-1][4:6]}-{days[-1][6:]}")
        else:
            print(f"âŒ {tn}ï¼šæ— ä»»ä½•èŠ‚ç›®")

    print("="*60 + "\n")

    # è¾“å‡º
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    out_path = os.path.join(OUTPUT_DIR, "epg.gz")

    root = etree.Element("tv")
    root.insert(0, etree.Comment(f"Built {datetime.now()} | çº¯å»é‡ï¼Œä¸è¿‡æ»¤ä»»ä½•æ—¶é—´"))
    for ch in all_channels:
        root.append(ch)
    for p in unique:
        root.append(p)

    xml_data = etree.tostring(root, encoding="utf-8", pretty_print=True, xml_declaration=True)
    with gzip.open(out_path, "wb") as f:
        f.write(xml_data)

    size = os.path.getsize(out_path) / 1024 / 1024
    print("="*60)
    print(f"âœ… ç”Ÿæˆå®Œæˆï¼ä»£ç å·²ä¸åšä»»ä½•æ—¥æœŸåˆ å‡")
    print(f"ğŸ“º é¢‘é“æ€»æ•°ï¼š{len(all_channels)}")
    print(f"ğŸ“… æœ‰æ•ˆèŠ‚ç›®ï¼š{len(unique)}")
    print(f"ğŸ“¦ æ–‡ä»¶å¤§å°ï¼š{size:.2f}MB")
    print("="*60)

if __name__ == "__main__":
    merge_all("weifang.gz")
