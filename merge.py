import os
import gzip
import re
import time
import signal
import requests
from lxml import etree
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor

# 10åˆ†é’Ÿå¼ºåˆ¶ç»ˆæ­¢
signal.signal(signal.SIGALRM, lambda s, f: os._exit(0))
signal.alarm(600)

OUTPUT_DIR = "output"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ====================== æ½åŠ4é¢‘é“é…ç½® ======================
WEIFANG_CHANNELS = [
    ("æ½åŠæ–°é—»é¢‘é“", "https://m.tvsou.com/epg/db502561"),
    ("æ½åŠç»æµç”Ÿæ´»é¢‘é“", "https://m.tvsou.com/epg/47a9d24a"),
    ("æ½åŠç§‘æ•™é¢‘é“", "https://m.tvsou.com/epg/d131d3d1"),
    ("æ½åŠå…¬å…±é¢‘é“", "https://m.tvsou.com/epg/c06f0cc0")
]

# ç½‘ç«™å›ºå®šåç¼€ï¼šå‘¨ä¸€w1 ~ å‘¨æ—¥w7
WEEK_MAP = {
    "å‘¨ä¸€": "w1",
    "å‘¨äºŒ": "w2",
    "å‘¨ä¸‰": "w3",
    "å‘¨å››": "w4",
    "å‘¨äº”": "w5",
    "å‘¨å…­": "w6",
    "å‘¨æ—¥": "w7"
}

MAX_RETRY = 2

# === å¿…åº”Referer é˜²åçˆ¬ ===
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Linux; Android 12; Mobile) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Mobile Safari/537.36",
    "Referer": "https://www.bing.com/search?q=%E7%94%B5%E8%A7%86%E8%8A%82%E7%9B%AE%E8%A1%A8"
}

# --- å¯é€‰Seleniumï¼ˆä¸è£…ä¹Ÿèƒ½è·‘ï¼‰---
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

# ====================== å·¥å…·å‡½æ•° ======================
def time_to_xmltv(base_date, time_str):
    try:
        hh, mm = time_str.strip().split(":")
        dt = datetime.combine(base_date, datetime.min.time().replace(hour=int(hh), minute=int(mm)))
        return dt.strftime("%Y%m%d%H%M%S +0800")
    except:
        return ""

def get_page_html(url):
    try:
        resp = requests.get(url, headers=HEADERS, timeout=12)
        resp.encoding = 'utf-8'
        html_content = resp.text
        if re.findall(r'\d{1,2}:\d{2}', html_content):
            return html_content
    except Exception:
        pass

    if SELENIUM_AVAILABLE:
        try:
            opt = Options()
            opt.add_argument("--headless")
            opt.add_argument("--no-sandbox")
            opt.add_argument("--disable-dev-shm-usage")
            opt.add_argument(f"user-agent={HEADERS['User-Agent']}")
            driver = webdriver.Chrome(options=opt)
            driver.get(url)
            time.sleep(2.5)
            html_content = driver.page_source
            driver.quit()
            return html_content
        except Exception:
            pass
    return ""

# ====================== æ ¸å¿ƒï¼šæŠ“ã€æœ¬å‘¨ä¸€ ~ æœ¬å‘¨æ—¥ã€‘7å¤© ======================
def get_channel_7days(channel_name, base_url):
    week_list = list(WEEK_MAP.items())
    today = datetime.now()
    monday = today - timedelta(days=today.weekday())
    channel_progs = []

    for i, (week_name, w_suffix) in enumerate(week_list):
        current_date = monday + timedelta(days=i)

        if base_url.endswith('/'):
            url = f"{base_url}{w_suffix}"
        else:
            url = f"{base_url}/{w_suffix}"

        html_content = get_page_html(url)
        if not html_content:
            time.sleep(1)
            continue

        soup = BeautifulSoup(html_content, "html.parser")
        items = soup.find_all("div", class_=re.compile("program-item|time-item", re.I))
        if not items:
            items = soup.find_all("li")

        day_progs = []
        for item in items:
            txt = item.get_text(strip=True)
            match = re.search(r'(\d{1,2}:\d{2})\s*(.+)', txt)
            if not match:
                continue
            t_str, title = match.groups()
            if len(title) < 2 or 'å¹¿å‘Š' in title or 'æŠ¥æ—¶' in title:
                continue
            day_progs.append((t_str.strip(), title.strip()))

        day_progs = sorted(list(set(day_progs)), key=lambda x: x[0])
        for idx in range(len(day_progs)):
            t_start, title = day_progs[idx]
            if idx < len(day_progs)-1:
                t_end = day_progs[idx+1][0]
            else:
                h, m = map(int, t_start.split(':'))
                end_dt = datetime(2000, 1, 1, h, m) + timedelta(minutes=30)
                t_end = end_dt.strftime("%H:%M")

            start = time_to_xmltv(current_date, t_start)
            end = time_to_xmltv(current_date, t_end)
            if start and end:
                channel_progs.append((start, end, title))
        time.sleep(1.0)
    return channel_progs

# ====================== æ½åŠ7å¤©æŠ“å– ======================
def crawl_weifang():
    try:
        root = etree.Element("tv")
        for ch_name, _ in WEIFANG_CHANNELS:
            ch = etree.SubElement(root, "channel", id=ch_name)
            dn = etree.SubElement(ch, "display-name", lang="zh")
            dn.text = ch_name

        for ch_name, base_url in WEIFANG_CHANNELS:
            programs = get_channel_7days(channel_name=ch_name, base_url=base_url)
            for start, stop, title in programs:
                prog = etree.SubElement(root, "programme", start=start, stop=stop, channel=ch_name)
                t = etree.SubElement(prog, "title", lang="zh")
                t.text = title

        wf_path = os.path.join(OUTPUT_DIR, "weifang.gz")
        xml_content = etree.tostring(root, encoding="utf-8", pretty_print=True, xml_declaration=True)
        
        with gzip.open(wf_path, "wb") as f:
            f.write(xml_content)
            
        print(f"âœ… æ½åŠEPGå·²ä¿å­˜: {wf_path}")
        return wf_path
        
    except Exception as e:
        print(f"âŒ æ½åŠæºæŠ“å–å¤±è´¥: {e}")
        wf_path = os.path.join(OUTPUT_DIR, "weifang.gz")
        empty_xml = b'<?xml version="1.0" encoding="utf-8"?>\n<tv></tv>'
        with gzip.open(wf_path, "wb") as f:
            f.write(empty_xml)
        return wf_path

# ====================== åŸæœ‰åˆå¹¶é€»è¾‘ ======================
def fetch_with_retry(u, max_retry=MAX_RETRY):
    for attempt in range(1, max_retry + 1):
        try:
            r = requests.get(u, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
            if r.status_code not in (200, 206):
                time.sleep(1)
                continue

            if u.endswith(".gz"):
                content = gzip.decompress(r.content).decode("utf-8", "ignore")
            else:
                content = r.text

            content = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f]', '', content).replace("& ", "&amp; ")
            tree = etree.fromstring(content.encode("utf-8"))
            ch = len(tree.xpath("//channel"))
            pg = len(tree.xpath("//programme"))
            if ch > 0 and pg > 0:
                return (True, tree, ch, pg, attempt)
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥ {u[:50]}...: {e}")
            time.sleep(1)
            continue
    return (False, None, 0, 0, max_retry)

def merge_all(weifang_gz_file):
    all_channels = []
    all_programs = []
    total_ch = 0
    total_pg = 0
    success_cnt = 0
    fail_cnt = 0

    if not os.path.exists("config.txt"):
        print("âŒ æœªæ‰¾åˆ° config.txt æ–‡ä»¶")
        return

    with open("config.txt", "r", encoding="utf-8") as f:
        urls = [l.strip() for l in f if l.strip() and l.startswith("http")]

    if not urls:
        print("âŒ config.txt ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„URL")
        return

    print("=" * 60)
    print("EPG æºæŠ“å–ç»Ÿè®¡ï¼ˆå¤±è´¥è‡ªåŠ¨é‡è¯•ï¼‰")
    print("=" * 60)

    with ThreadPoolExecutor(max_workers=6) as executor:
        future_map = {executor.submit(fetch_with_retry, u): u for u in urls}
        for fut in future_map:
            u = future_map[fut]
            ok, tree, ch, pg, retry_cnt = fut.result()
            if ok:
                success_cnt += 1
                total_ch += ch
                total_pg += pg
                log_retry = f"[é‡è¯•{retry_cnt-1}æ¬¡]" if retry_cnt > 1 else ""
                print(f"âœ… {u[:55]}... {log_retry}æˆåŠŸ | é¢‘é“ {ch:>4} | èŠ‚ç›® {pg:>6}")
                for node in tree:
                    if node.tag == "channel":
                        all_channels.append(node)
                    elif node.tag == "programme":
                        all_programs.append(node)
            else:
                fail_cnt += 1

    if fail_cnt > 0:
        print(f"âŒ å…± {fail_cnt} ä¸ªæºç»{MAX_RETRY}æ¬¡é‡è¯•åä»å¤±è´¥ï¼Œå·²è·³è¿‡")

    print("=" * 60)
    print(f"æ±‡æ€»ï¼šæˆåŠŸ {success_cnt} ä¸ª | å¤±è´¥ {fail_cnt} ä¸ª | æ€»é¢‘é“ {total_ch} | æ€»èŠ‚ç›® {total_pg}")
    print("=" * 60)

    try:
        with gzip.open(weifang_gz_file, "rb") as f:
            wf_content = f.read().decode("utf-8")
            wf_tree = etree.fromstring(wf_content.encode("utf-8"))
            wf_ch = len(wf_tree.xpath("//channel"))
            wf_pg = len(wf_tree.xpath("//programme"))

        if wf_ch > 0 and wf_pg > 0:
            print(f"ğŸ“º æ½åŠæœ¬åœ°æºï¼šé¢‘é“ {wf_ch} | èŠ‚ç›® {wf_pg}ï¼ˆæœ¬å‘¨ä¸€~å‘¨æ—¥å®Œæ•´7å¤©ï¼‰")
            for node in wf_tree:
                if node.tag == "channel":
                    all_channels.append(node)
                elif node.tag == "programme":
                    all_programs.append(node)
        else:
            print("âš ï¸ æ½åŠæœ¬åœ°æºæŠ“å–å¤±è´¥ï¼Œå·²è·³è¿‡")
    except Exception as e:
        print(f"âš ï¸ æ½åŠæœ¬åœ°æºè¯»å–å¤±è´¥: {e}")

    print(f"å¤„ç†å‰: é¢‘é“ {len(all_channels)} ä¸ª, èŠ‚ç›® {len(all_programs)} ä¸ª")

    # ====================== ä¿®å¤ï¼šé¢‘é“å»é‡ ======================
    seen_channel_names = set()
    unique_channels = []
    channel_id_mapping = {}  # å­˜å‚¨åŸå§‹é¢‘é“IDåˆ°ä¿ç•™é¢‘é“IDçš„æ˜ å°„
    channel_name_to_id = {}  # å­˜å‚¨é¢‘é“åç§°åˆ°ä¿ç•™é¢‘é“IDçš„æ˜ å°„
    
    for ch in all_channels:
        # è·å–é¢‘é“åç§°ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        display_name_node = ch.find("display-name")
        if display_name_node is not None and display_name_node.text:
            channel_name = display_name_node.text.strip()
            channel_name_lower = channel_name.lower()  # è½¬æ¢ä¸ºå°å†™è¿›è¡Œä¸åŒºåˆ†å¤§å°å†™çš„æ¯”è¾ƒ
            
            # è·å–é¢‘é“ID
            channel_id = ch.get('id', '')
            
            if channel_name_lower not in seen_channel_names:
                # ç¬¬ä¸€æ¬¡å‡ºç°è¿™ä¸ªé¢‘é“åç§°ï¼Œä¿ç•™å®ƒ
                seen_channel_names.add(channel_name_lower)
                unique_channels.append(ch)
                
                # è®°å½•è¿™ä¸ªé¢‘é“åç§°å¯¹åº”çš„IDï¼ˆä¿ç•™é¢‘é“çš„IDï¼‰
                if channel_id:
                    channel_id_mapping[channel_name_lower] = channel_id
                    channel_name_to_id[channel_name_lower] = channel_id
            else:
                # é‡å¤çš„é¢‘é“åç§°ï¼Œè·³è¿‡ä¸ä¿ç•™
                # ä½†éœ€è¦è®°å½•è¿™ä¸ªé¢‘é“çš„IDæ˜ å°„å…³ç³»ï¼Œä»¥ä¾¿åç»­æ›´æ–°èŠ‚ç›®
                if channel_id and channel_name_lower in channel_id_mapping:
                    # è®°å½•é‡å¤é¢‘é“çš„IDåˆ°ä¿ç•™é¢‘é“IDçš„æ˜ å°„
                    retained_id = channel_id_mapping[channel_name_lower]
                    channel_id_mapping[channel_id] = retained_id
        else:
            # æ²¡æœ‰display-nameçš„é¢‘é“ï¼Œç›´æ¥ä¿ç•™
            unique_channels.append(ch)
    
    print(f"é¢‘é“å»é‡å: {len(unique_channels)} ä¸ªå”¯ä¸€é¢‘é“")
    
    # ====================== æ™ºèƒ½èŠ‚ç›®å»é‡ ======================
    # ä½¿ç”¨å­—å…¸å­˜å‚¨èŠ‚ç›®ï¼Œé”®ä¸º (channel_id, start_time, title) çš„å…ƒç»„
    program_dict = {}
    
    for prog in all_programs:
        try:
            old_channel_id = prog.get('channel')
            if not old_channel_id:
                continue
                
            start_time = prog.get('start')
            stop_time = prog.get('stop')
            title_elem = prog.find("title")
            
            if not start_time or not stop_time or title_elem is None:
                continue
                
            title = title_elem.text.strip() if title_elem.text else ""
            if not title or len(title) < 2:
                continue
                
            # æŸ¥æ‰¾æ­£ç¡®çš„é¢‘é“ID
            new_channel_id = old_channel_id
            # å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ç›´æ¥æ˜ å°„
            if old_channel_id in channel_id_mapping:
                new_channel_id = channel_id_mapping[old_channel_id]
            else:
                # æ£€æŸ¥æ˜¯å¦æœ‰é€šè¿‡é¢‘é“åç§°çš„æ˜ å°„
                for ch_name_lower, ch_id in channel_name_to_id.items():
                    if old_channel_id.lower() in ch_name_lower or ch_name_lower in old_channel_id.lower():
                        new_channel_id = ch_id
                        break
            
            # åˆ›å»ºèŠ‚ç›®é”®
            program_key = (new_channel_id, start_time, title)
            
            # å¦‚æœè¿™ä¸ªèŠ‚ç›®å·²ç»å­˜åœ¨ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆä¿ç•™æ›´é•¿çš„èŠ‚ç›®æ—¶é—´ï¼‰
            if program_key in program_dict:
                existing_prog = program_dict[program_key]
                existing_stop = existing_prog.get('stop')
                # å¦‚æœæ–°èŠ‚ç›®çš„ç»“æŸæ—¶é—´æ›´æ™šï¼Œåˆ™æ›¿æ¢
                if stop_time > existing_stop:
                    prog.set('channel', new_channel_id)
                    program_dict[program_key] = prog
            else:
                # æ–°èŠ‚ç›®ï¼Œè®¾ç½®æ–°çš„é¢‘é“IDå¹¶å­˜å‚¨
                prog.set('channel', new_channel_id)
                program_dict[program_key] = prog
                
        except Exception as e:
            print(f"âš ï¸ å¤„ç†èŠ‚ç›®æ—¶å‡ºé”™: {e}")
            continue
    
    unique_programs = list(program_dict.values())
    print(f"èŠ‚ç›®å»é‡å: {len(unique_programs)} ä¸ªå”¯ä¸€èŠ‚ç›®")
    print(f"ğŸ¯ å»é‡ç‡: {(len(all_programs) - len(unique_programs)) / len(all_programs) * 100:.1f}%")
    
    # æŒ‰é¢‘é“å’Œå¼€å§‹æ—¶é—´æ’åºèŠ‚ç›®
    unique_programs.sort(key=lambda x: (x.get('channel', ''), x.get('start', '')))
    
    # ç”Ÿæˆæœ€ç»ˆXMLï¼ˆç”¨å»é‡åçš„é¢‘é“ + å»é‡åçš„èŠ‚ç›®ï¼‰
    final_root = etree.Element("tv")
    for ch in unique_channels:
        final_root.append(ch)
    for p in unique_programs:
        final_root.append(p)

    xml_str = etree.tostring(final_root, encoding="utf-8", pretty_print=True, xml_declaration=True)
    output_path = os.path.join(OUTPUT_DIR, "epg.gz")
    with gzip.open(output_path, "wb") as f:
        f.write(xml_str)
    
    # è®¡ç®—æ–‡ä»¶å¤§å°
    file_size_mb = os.path.getsize(output_path) / 1024 / 1024
    print(f"âœ… æœ€ç»ˆè¾“å‡ºï¼šé¢‘é“ {len(unique_channels)} ä¸ª | èŠ‚ç›® {len(unique_programs)} ä¸ª")
    print(f"ğŸ“¦ æ–‡ä»¶å¤§å°ï¼š{file_size_mb:.2f} MB")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶ï¼š{output_path}")
    print("=" * 60)
    
    # ä¿å­˜ä¸€ä»½æœªå‹ç¼©çš„XMLç”¨äºè°ƒè¯•
    xml_debug_path = os.path.join(OUTPUT_DIR, "epg.xml")
    with open(xml_debug_path, "wb") as f:
        f.write(xml_str)
    print(f"ğŸ“ è°ƒè¯•æ–‡ä»¶ï¼ˆæœªå‹ç¼©ï¼‰ï¼š{xml_debug_path}")
    print("=" * 60)

# ====================== å…¥å£ ======================
if __name__ == "__main__":
    try:
        print("å¼€å§‹æŠ“å–æ½åŠæœ¬åœ°EPG...")
        wf_gz = crawl_weifang()
        print("å¼€å§‹åˆå¹¶æ‰€æœ‰EPGæº...")
        merge_all(wf_gz)
        print("âœ… EPGåˆå¹¶å®Œæˆï¼")
    except Exception as e:
        print(f"âŒ è„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
