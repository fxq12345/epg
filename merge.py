import os
import gzip
import re
import time
import logging
from typing import List, Dict, Set, Tuple
from datetime import datetime
import requests
from lxml import etree
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ===================== ç‹¬ç«‹é…ç½®åŒº ===================import os
import gzip
import re
import time
import logging
from typing import List, Dict, Set, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
import requests
from lxml import etree
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ===================== é…ç½®åŒº =====================
CONFIG_FILE = "config.txt"
OUTPUT_DIR = "output"
LOG_FILE = "epg_merge.log"
MAX_WORKERS = 3
TIMEOUT = 30
CORE_RETRY_COUNT = 2
# æ–°å¢ï¼šæœ¬åœ°æ½åŠEPGæ–‡ä»¶è·¯å¾„ï¼ˆå¯¹åº”ä¹‹å‰ç”Ÿæˆçš„weifang.xmlï¼‰
LOCAL_WEIFANG_EPG = os.path.join(OUTPUT_DIR, "weifang.xml")

# é…ç½®æ—¥å¿—ï¼šæå‡æ—¥å¿—çº§åˆ«ä¸ºDEBUGï¼Œè¾“å‡ºæ›´è¯¦ç»†ä¿¡æ¯
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
# ==================================================

class EPGGenerator:
    def __init__(self):
        self.session = self._create_session()
        self.channel_ids: Set[str] = set()
        self.all_channels: List = []
        self.all_programs: List = []
        self.channel_programs: Dict[str, List] = {}  # é¢‘é“ID -> èŠ‚ç›®åˆ—è¡¨
        
    def _create_session(self) -> requests.Session:
        """åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„ä¼šè¯"""
        session = requests.Session()
        retry_strategy = Retry(
            total=CORE_RETRY_COUNT + 2,
            backoff_factor=1.5,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "application/xml, */*",
            "Accept-Encoding": "gzip, deflate"
        })
        return session

    def read_epg_sources(self) -> List[str]:
        """è¯»å–é…ç½®æ–‡ä»¶ä¸­çš„EPGæº"""
        if not os.path.exists(CONFIG_FILE):
            logging.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {CONFIG_FILE}")
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶: {CONFIG_FILE}")
            
        try:
            with open(CONFIG_FILE, "r", encoding="utf-8") as f:
                sources = []
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if line and not line.startswith("#") and line.startswith(("http://", "https://")):
                        sources.append(line)
                
                logging.info(f"è¯»å–åˆ°{len(sources)}ä¸ªEPGæº")
                for i, source in enumerate(sources, 1):
                    logging.info(f"  {i}. {source}")
                
                return sources
                
        except Exception as e:
            logging.error(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
            raise

    def clean_xml_content(self, content: str) -> str:
        """æ¸…ç†XMLå†…å®¹"""
        content_clean = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', content)
        content_clean = content_clean.replace('& ', '&amp; ')
        return content_clean

    def fetch_single_source(self, source: str) -> Tuple[bool, any]:
        """è·å–å•ä¸ªEPGæºæ•°æ®"""
        try:
            start_time = time.time()
            logging.info(f"æŠ“å–: {source[:60]}...")
            
            response = self.session.get(source, timeout=TIMEOUT)
            response.raise_for_status()
            
            if source.endswith('.gz'):
                content = gzip.decompress(response.content).decode('utf-8')
                logging.debug(f"æºæ–‡ä»¶ä¸ºGZIPæ ¼å¼ï¼Œå·²è§£å‹ï¼Œå¤§å°: {len(content)} å­—ç¬¦")
            else:
                content = response.text
                logging.debug(f"æºæ–‡ä»¶ä¸ºXMLæ ¼å¼ï¼Œå¤§å°: {len(content)} å­—ç¬¦")
                
            content_clean = self.clean_xml_content(content)
            xml_tree = etree.fromstring(content_clean.encode('utf-8'))
            
            cost_time = time.time() - start_time
            logging.info(f"æˆåŠŸ: {cost_time:.2f}s")
            return True, xml_tree
            
        except Exception as e:
            logging.error(f"å¤±è´¥: {str(e)[:80]}")
            return False, None

    def process_channels_and_programs(self, xml_tree):
        """å¤„ç†é¢‘é“å’ŒèŠ‚ç›®æ•°æ®"""
        # å¤„ç†é¢‘é“
        channels = xml_tree.xpath("//channel")
        logging.debug(f"æ‰¾åˆ° {len(channels)} ä¸ªé¢‘é“èŠ‚ç‚¹")
        for channel in channels:
            channel_id = channel.get("id", "").strip()
            if not channel_id:
                logging.debug(f"è·³è¿‡æ— IDçš„é¢‘é“èŠ‚ç‚¹")
                continue
            if channel_id in self.channel_ids:
                logging.debug(f"é¢‘é“ {channel_id} å·²å­˜åœ¨ï¼Œè·³è¿‡")
                continue
                
            self.channel_ids.add(channel_id)
            self.all_channels.append(channel)
            logging.debug(f"æ–°å¢é¢‘é“: {channel_id}")
            
            # åˆå§‹åŒ–è¯¥é¢‘é“çš„èŠ‚ç›®åˆ—è¡¨
            if channel_id not in self.channel_programs:
                self.channel_programs[channel_id] = []
        
        # å¤„ç†èŠ‚ç›®
        programs = xml_tree.xpath("//programme")
        logging.debug(f"æ‰¾åˆ° {len(programs)} ä¸ªèŠ‚ç›®èŠ‚ç‚¹")
        for program in programs:
            channel_id = program.get("channel", "").strip()
            if channel_id and channel_id in self.channel_programs:
                self.channel_programs[channel_id].append(program)
                self.all_programs.append(program)
                logging.debug(f"ä¸ºé¢‘é“ {channel_id} æ·»åŠ èŠ‚ç›®: {program.find('title').text if program.find('title') is not None else 'æ— æ ‡é¢˜'}")

    # æ–°å¢ï¼šå¤„ç†æœ¬åœ°æ½åŠEPGæ–‡ä»¶
    def process_local_weifang_epg(self):
        """è¯»å–å¹¶å¤„ç†æœ¬åœ°çš„æ½åŠEPGæ–‡ä»¶"""
        if not os.path.exists(LOCAL_WEIFANG_EPG):
            logging.warning(f"æœ¬åœ°æ½åŠEPGæ–‡ä»¶ä¸å­˜åœ¨: {LOCAL_WEIFANG_EPG}ï¼Œè·³è¿‡åˆå¹¶")
            return
        
        try:
            logging.info(f"å¼€å§‹åˆå¹¶æœ¬åœ°æ½åŠEPGæ–‡ä»¶: {LOCAL_WEIFANG_EPG}")
            with open(LOCAL_WEIFANG_EPG, "r", encoding="utf-8") as f:
                content = f.read()
            content_clean = self.clean_xml_content(content)
            xml_tree = etree.fromstring(content_clean.encode('utf-8'))
            self.process_channels_and_programs(xml_tree)
            logging.info(f"âœ… æˆåŠŸåˆå¹¶æœ¬åœ°æ½åŠEPGæ–‡ä»¶")
        except Exception as e:
            logging.error(f"åˆå¹¶æœ¬åœ°æ½åŠEPGå¤±è´¥: {str(e)}")

    def fetch_and_process_all_sources(self, sources: List[str]) -> bool:
        """è·å–å¹¶å¤„ç†æ‰€æœ‰EPGæº"""
        successful_sources = 0
        
        with ThreadPoolExecutor(max_workers=min(MAX_WORKERS, len(sources))) as executor:
            future_to_source = {executor.submit(self.fetch_single_source, source): source 
                              for source in sources}
            
            for future in as_completed(future_to_source):
                source = future_to_source[future]
                try:
                    success, xml_tree = future.result()
                    if success and xml_tree is not None:
                        self.process_channels_and_programs(xml_tree)
                        successful_sources += 1
                        logging.info(f"âœ… æˆåŠŸå¤„ç†æº: {source[:60]}...")
                except Exception as e:
                    logging.error(f"å¤„ç†å¤±è´¥ {source}: {str(e)[:80]}")
        
        # æ–°å¢ï¼šå¤„ç†æœ¬åœ°æ½åŠEPG
        self.process_local_weifang_epg()
        
        return successful_sources > 0

    def generate_final_xml(self) -> str:
        """ç”Ÿæˆæœ€ç»ˆçš„EPG XMLæ–‡ä»¶"""
        xml_declare = f'''<?xml version="1.0" encoding="UTF-8"?>
<tv generator-info-name="EPGåˆå¹¶å™¨" 
    last-update="{datetime.now().strftime('%Y%m%d%H%M%S')}">'''
        
        root = etree.fromstring(f"{xml_declare}</tv>".encode("utf-8"))
        
        # æ·»åŠ æ‰€æœ‰é¢‘é“
        logging.debug(f"å¼€å§‹æ·»åŠ  {len(self.all_channels)} ä¸ªé¢‘é“åˆ°æœ€ç»ˆXML")
        for channel in self.all_channels:
            root.append(channel)
            
        # æ·»åŠ æ‰€æœ‰èŠ‚ç›®å•
        logging.debug(f"å¼€å§‹æ·»åŠ  {len(self.all_programs)} ä¸ªèŠ‚ç›®åˆ°æœ€ç»ˆXML")
        for program in self.all_programs:
            root.append(program)
            
        return etree.tostring(root, encoding="utf-8", pretty_print=True).decode("utf-8")

    def save_files(self, xml_content: str):
        """ä¿å­˜EPGæ–‡ä»¶"""
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        
        # æ¸…ç†æ—§æ–‡ä»¶
        for f in os.listdir(OUTPUT_DIR):
            if f.endswith(('.xml', '.gz')) and os.path.isfile(os.path.join(OUTPUT_DIR, f)):
                try:
                    os.remove(os.path.join(OUTPUT_DIR, f))
                    logging.debug(f"åˆ é™¤æ—§æ–‡ä»¶: {f}")
                except Exception:
                    pass
        
        # ä¿å­˜XMLæ–‡ä»¶
        xml_path = os.path.join(OUTPUT_DIR, "epg.xml")
        with open(xml_path, "w", encoding="utf-8") as f:
            f.write(xml_content)
        xml_size = os.path.getsize(xml_path) / 1024 / 1024  # MB
        
        # ä¿å­˜GZIPå‹ç¼©æ–‡ä»¶
        gz_path = os.path.join(OUTPUT_DIR, "epg.gz")
        with gzip.open(gz_path, "wb") as f:
            f.write(xml_content.encode("utf-8"))
        gz_size = os.path.getsize(gz_path) / 1024  # KB
        
        logging.info(f"ğŸ’¾ æ–‡ä»¶ä¿å­˜æˆåŠŸ:")
        logging.info(f"  ğŸ“„ epg.xml: {xml_size:.2f} MB")
        logging.info(f"  ğŸ“¦ epg.gz: {gz_size:.1f} KB")

    def print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        total_channels = len(self.channel_ids)
        total_programs = len(self.all_programs)
        
        logging.info("\n" + "="*50)
        logging.info("ğŸ“Š EPGç»Ÿè®¡æŠ¥å‘Š")
        logging.info("="*50)
        logging.info(f"æ€»é¢‘é“æ•°: {total_channels}")
        logging.info(f"æ€»èŠ‚ç›®æ•°: {total_programs}")
        
        # æ˜¾ç¤ºæ²¡æœ‰èŠ‚ç›®å•çš„é¢‘é“
        channels_without_programs = [c for c in self.channel_ids 
                                   if c not in self.channel_programs or not self.channel_programs[c]]
        if channels_without_programs:
            logging.info(f"æ— èŠ‚ç›®å•çš„é¢‘é“: {len(channels_without_programs)}ä¸ª")
            for channel in channels_without_programs[:20]:  # æ˜¾ç¤ºå‰20ä¸ª
                logging.info(f"  - {channel}")
        
        logging.info("="*50)

    def run(self):
        """ä¸»è¿è¡Œæ–¹æ³•"""
        start_time = time.time()
        logging.info("ğŸš€ å¼€å§‹EPGåˆå¹¶")
        
        try:
            # è¯»å–EPGæº
            sources = self.read_epg_sources()
            if not sources:
                logging.error("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„EPGæº")
                return False
            
            # è·å–å¹¶å¤„ç†æ‰€æœ‰æºï¼ˆå«æœ¬åœ°æ½åŠEPGï¼‰
            if not self.fetch_and_process_all_sources(sources):
                logging.error("âŒ EPGæºè·å–å¤±è´¥")
                return False
                
            # ç”Ÿæˆæœ€ç»ˆXML
            xml_content = self.generate_final_xml()
            
            # ä¿å­˜æ–‡ä»¶
            self.save_files(xml_content)
            
            # æ‰“å°ç»Ÿè®¡
            self.print_statistics()
            
            total_time = time.time() - start_time
            logging.info(f"âœ… å®Œæˆ! è€—æ—¶: {total_time:.2f}ç§’")
            return True
            
        except Exception as e:
            logging.error(f"ğŸ’¥ å¤±è´¥: {str(e)}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*50)
    print("ğŸ“º EPGåˆå¹¶å·¥å…·")
    print("="*50)
    
    generator = EPGGenerator()
    success = generator.run()
    
    if success:
        print("\nâœ… EPGæ–‡ä»¶ç”ŸæˆæˆåŠŸ!")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {os.path.abspath(OUTPUT_DIR)}")
    else:
        print("\nâŒ EPGæ–‡ä»¶ç”Ÿæˆå¤±è´¥!")
    
    exit(0 if success else 1)

if __name__ == "__main__":
    main()
==
OUTPUT_DIR = "output"
LOG_FILE = "epg_merge.log"
TIMEOUT = 30
# æ½åŠæœ¬åœ°EPGæŠ“å–æ¥å£ï¼ˆé›†æˆåˆ°mergeè„šæœ¬ä¸­ï¼‰
WEIFANG_CHANNELS = [
    {"id": "1001", "name": "æ½åŠæ–°é—»ç»¼åˆé¢‘é“", "alias": "æ½åŠæ–°é—»"},
    {"id": "1002", "name": "æ½åŠç»æµç”Ÿæ´»é¢‘é“", "alias": "æ½åŠç»æµç”Ÿæ´»"},
    {"id": "1003", "name": "æ½åŠå…¬å…±é¢‘é“", "alias": "æ½åŠå…¬å…±"},
    {"id": "1004", "name": "æ½åŠç§‘æ•™æ–‡åŒ–é¢‘é“", "alias": "æ½åŠç§‘æ•™æ–‡åŒ–"},
    {"id": "1008", "name": "å¯¿å…‰è”¬èœé¢‘é“", "alias": "å¯¿å…‰è”¬èœ"},
    {"id": "1009", "name": "æ˜Œä¹ç»¼åˆé¢‘é“", "alias": "æ˜Œä¹ç»¼åˆ"},
    {"id": "1011", "name": "å¥æ–‡å¨±ä¹é¢‘é“", "alias": "å¥æ–‡å¨±ä¹"}
]
# å¤–éƒ¨EPGæºï¼ˆå¯ç›´æ¥å†™åœ¨è„šæœ¬å†…ï¼Œæ— éœ€config.txtï¼‰
EXTERNAL_EPG_SOURCES = [
    # ç¤ºä¾‹ï¼šæ·»åŠ ä½ éœ€è¦çš„å¤–éƒ¨EPGæº
    "https://example.com/epg.xml"
]
# ==================================================

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

class IndependentEPGMerger:
    def __init__(self):
        self.session = self._create_session()
        self.channel_ids: Set[str] = set()
        self.all_channels: List = []
        self.all_programs: List = []
        self.channel_programs: Dict[str, List] = {}
        os.makedirs(OUTPUT_DIR, exist_ok=True)

    def _create_session(self) -> requests.Session:
        """åˆ›å»ºå¸¦é‡è¯•çš„ä¼šè¯"""
        session = requests.Session()
        retry = Retry(total=3, backoff_factor=1.5, status_forcelist=[429, 500, 502, 503, 504])
        session.mount("http://", HTTPAdapter(max_retries=retry))
        session.mount("https://", HTTPAdapter(max_retries=retry))
        session.headers.update({"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"})
        return session

    def _clean_xml(self, content: str) -> str:
        """æ¸…ç†XMLéæ³•å­—ç¬¦"""
        return re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', content).replace('& ', '&amp; ')

    # ========== æ–°å¢ï¼šé›†æˆæ½åŠæœ¬åœ°EPGæŠ“å– ==========
    def _fetch_weifang_epg(self) -> etree._Element:
        """ç›´æ¥æŠ“å–æ½åŠæœ¬åœ°EPGå¹¶ç”ŸæˆXMLæ ‘"""
        logging.info("å¼€å§‹æŠ“å–æ½åŠæœ¬åœ°EPG...")
        programmes = []
        headers = {"User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1"}

        # åˆ›å»ºæ ¹èŠ‚ç‚¹
        root = etree.Element("tv")

        # æ·»åŠ æ½åŠé¢‘é“
        for channel in WEIFANG_CHANNELS:
            channel_elem = etree.SubElement(root, "channel")
            channel_elem.set("id", channel["id"])
            etree.SubElement(channel_elem, "display-name", lang="zh-CN").text = channel["name"]
            etree.SubElement(channel_elem, "display-name", lang="zh-CN").text = channel["alias"]

        # æŠ“å–3å¤©èŠ‚ç›®
        for day_offset in range(3):
            target_date = (datetime.today() + datetime.timedelta(days=day_offset)).strftime("%Y-%m-%d")
            for channel in WEIFANG_CHANNELS:
                try:
                    url = f"https://sd.iqilu.com/api/tv/program?channel={channel['alias']}&date={target_date}"
                    resp = self.session.get(url, headers=headers, timeout=TIMEOUT)
                    resp.raise_for_status()
                    data = resp.json()

                    for prog in data.get("data", []):
                        # è½¬æ¢æ—¶é—´æ ¼å¼
                        start = f"{prog['start_time'].replace('-', '').replace(':', '')} +0800"
                        stop = f"{prog['end_time'].replace('-', '').replace(':', '')} +0800"
                        
                        # åˆ›å»ºèŠ‚ç›®èŠ‚ç‚¹
                        prog_elem = etree.SubElement(root, "programme", channel=channel["id"], start=start, stop=stop)
                        etree.SubElement(prog_elem, "title", lang="zh-CN").text = prog["program_name"]
                        if prog.get("program_desc"):
                            etree.SubElement(prog_elem, "desc", lang="zh-CN").text = prog["program_desc"]

                except Exception as e:
                    logging.warning(f"æŠ“å–{channel['name']}èŠ‚ç›®å¤±è´¥: {str(e)}")

        logging.info("æ½åŠæœ¬åœ°EPGæŠ“å–å®Œæˆ")
        return root
    # ==============================================

    def _fetch_external_epg(self, source: str) -> etree._Element:
        """æŠ“å–å¤–éƒ¨EPGæº"""
        try:
            resp = self.session.get(source, timeout=TIMEOUT)
            resp.raise_for_status()
            content = gzip.decompress(resp.content).decode('utf-8') if source.endswith('.gz') else resp.text
            return etree.fromstring(self._clean_xml(content).encode('utf-8'))
        except Exception as e:
            logging.error(f"å¤–éƒ¨EPGæº{source}æŠ“å–å¤±è´¥: {str(e)}")
            return etree.Element("tv")  # è¿”å›ç©ºèŠ‚ç‚¹

    def _merge_epg(self, xml_trees: List[etree._Element]):
        """åˆå¹¶æ‰€æœ‰EPGæ•°æ®"""
        # åˆ›å»ºæœ€ç»ˆæ ¹èŠ‚ç‚¹
        final_root = etree.Element("tv", generator_info_name="ç‹¬ç«‹EPGåˆå¹¶å™¨", last_update=datetime.now().strftime("%Y%m%d%H%M%S"))

        # åˆå¹¶é¢‘é“å’ŒèŠ‚ç›®
        for tree in xml_trees:
            # åˆå¹¶é¢‘é“
            for channel in tree.xpath("//channel"):
                channel_id = channel.get("id")
                if channel_id not in self.channel_ids:
                    self.channel_ids.add(channel_id)
                    final_root.append(channel)
            
            # åˆå¹¶èŠ‚ç›®
            for program in tree.xpath("//programme"):
                final_root.append(program)

        return final_root

    def _save_epg(self, xml_root: etree._Element):
        """ä¿å­˜XMLå’ŒGZIPæ–‡ä»¶"""
        xml_content = etree.tostring(xml_root, encoding="utf-8", pretty_print=True).decode("utf-8")
        
        # ä¿å­˜XML
        xml_path = os.path.join(OUTPUT_DIR, "epg.xml")
        with open(xml_path, "w", encoding="utf-8") as f:
            f.write(xml_content)
        
        # ä¿å­˜GZIP
        gz_path = os.path.join(OUTPUT_DIR, "epg.gz")
        with gzip.open(gz_path, "wb") as f:
            f.write(xml_content.encode("utf-8"))

        logging.info(f"EPGæ–‡ä»¶å·²ä¿å­˜è‡³{OUTPUT_DIR}")

    def run(self):
        """ä¸»è¿è¡Œé€»è¾‘"""
        start_time = time.time()
        logging.info("ç‹¬ç«‹EPGåˆå¹¶å¼€å§‹")

        # 1. æŠ“å–æ½åŠæœ¬åœ°EPG
        weifang_tree = self._fetch_weifang_epg()

        # 2. æŠ“å–å¤–éƒ¨EPGæº
        external_trees = [self._fetch_external_epg(source) for source in EXTERNAL_EPG_SOURCES]

        # 3. åˆå¹¶æ‰€æœ‰EPG
        all_trees = [weifang_tree] + external_trees
        final_tree = self._merge_epg(all_trees)

        # 4. ä¿å­˜æ–‡ä»¶
        self._save_epg(final_tree)

        logging.info(f"åˆå¹¶å®Œæˆï¼Œè€—æ—¶{time.time() - start_time:.2f}ç§’")
        return True

if __name__ == "__main__":
    merger = IndependentEPGMerger()
    merger.run()
