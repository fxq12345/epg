import os
import gzip
import re
import time
import signal
import requests
from lxml import etree
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor

# 10åˆ†é’Ÿå¼ºåˆ¶ç»ˆæ­¢
signal.signal(signal.SIGALRM, lambda s, f: os._exit(0))
signal.alarm(600)

OUTPUT_DIR = "output"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ====================== æ½åŠ4é¢‘é“é…ç½®ï¼šå”¯ä¸€IDï¼ˆWF-å‰ç¼€ï¼‰ï¼Œé¿å…ä¸å¤®è§†/ä¸Šæ¸¸å†²çª ======================
WEIFANG_CHANNELS = [
    ("WF-æ–°é—»", "æ½åŠæ–°é—»é¢‘é“", "https://m.tvsou.com/epg/db502561"),
    ("WF-ç»æµ", "æ½åŠç»æµç”Ÿæ´»é¢‘é“", "https://m.tvsou.com/epg/47a9d24a"),
    ("WF-ç§‘æ•™", "æ½åŠç§‘æ•™é¢‘é“", "https://m.tvsou.com/epg/d131d3d1"),
    ("WF-å…¬å…±", "æ½åŠå…¬å…±é¢‘é“", "https://m.tvsou.com/epg/c06f0cc0")
]

# æ½åŠé¢‘é“æ˜¾ç¤ºååˆ—è¡¨ï¼Œç”¨äºè¿‡æ»¤ä¸Šæ¸¸æºä¸­çš„åŒåé¢‘é“
WEIFANG_DISPLAY_NAMES = {name for _, name, _ in WEIFANG_CHANNELS}

WEEK_MAP = {
    "å‘¨ä¸€": "w1", "å‘¨äºŒ": "w2", "å‘¨ä¸‰": "w3", "å‘¨å››": "w4",
    "å‘¨äº”": "w5", "å‘¨å…­": "w6", "å‘¨æ—¥": "w7"
}

MAX_RETRY = 2

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Linux; Android 12; Mobile) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Mobile Safari/537.36",
    "Referer": "https://www.bing.com"
}

# ====================== å·¥å…·å‡½æ•° ======================
def time_to_xmltv(base_date, time_str):
    try:
        hh, mm = time_str.strip().split(":")
        dt = datetime.combine(base_date, datetime.min.time().replace(hour=int(hh), minute=int(mm)))
        return dt.strftime("%Y%m%d%H%M%S +0800")
    except:
        return ""

def get_page_html(url):
    try:
        resp = requests.get(url, headers=HEADERS, timeout=12)
        resp.encoding = 'utf-8'
        if re.findall(r'\d{1,2}:\d{2}', resp.text):
            return resp.text
    except:
        pass
    return ""

# ====================== æŠ“å–æ½åŠ7å¤© ======================
def get_channel_7days(channel_id, channel_name, base_url):
    week_list = list(WEEK_MAP.items())
    today = datetime.now()
    monday = today - timedelta(days=today.weekday())
    channel_progs = []

    for i, (week_name, w_suffix) in enumerate(week_list):
        current_date = monday + timedelta(days=i)
        url = f"{base_url}/{w_suffix}" if not base_url.endswith('/') else f"{base_url}{w_suffix}"
        html = get_page_html(url)
        if not html:
            time.sleep(1)
            continue

        soup = BeautifulSoup(html, "html.parser")
        items = soup.find_all("div", class_=re.compile("program-item|time-item", re.I)) or soup.find_all("li")
        day_progs = []
        for item in items:
            txt = item.get_text(strip=True)
            match = re.search(r'(\d{1,2}:\d{2})\s*(.+)', txt)
            if not match:
                continue
            t_str, title = match.groups()
            if len(title) < 2 or 'å¹¿å‘Š' in title or 'æŠ¥æ—¶' in title:
                continue
            day_progs.append((t_str.strip(), title.strip()))

        day_progs = sorted(list(set(day_progs)), key=lambda x: x[0])
        for idx in range(len(day_progs)):
            t_start, title = day_progs[idx]
            t_end = day_progs[idx+1][0] if idx < len(day_progs)-1 else (datetime(2000,1,1,*map(int, t_start.split(':')))+timedelta(minutes=30)).strftime("%H:%M")

            start = time_to_xmltv(current_date, t_start)
            end = time_to_xmltv(current_date, t_end)
            if start and end:
                channel_progs.append((start, end, title))
        time.sleep(1)
    return channel_progs

def crawl_weifang():
    try:
        root = etree.Element("tv")
        # ç”Ÿæˆæ½åŠé¢‘é“çš„å”¯ä¸€IDé…ç½®
        for ch_id, ch_name, _ in WEIFANG_CHANNELS:
            ch = etree.SubElement(root, "channel", id=ch_id)
            dn = etree.SubElement(ch, "display-name", lang="zh")
            dn.text = ch_name

        # æŠ“å–æ½åŠèŠ‚ç›®
        for ch_id, ch_name, base_url in WEIFANG_CHANNELS:
            progs = get_channel_7days(ch_id, ch_name, base_url)
            for s, e, t in progs:
                p = etree.SubElement(root, "programme", start=s, stop=e, channel=ch_id)
                te = etree.SubElement(p, "title", lang="zh")
                te.text = t

        wf_path = os.path.join(OUTPUT_DIR, "weifang.gz")
        xml_content = etree.tostring(root, encoding="utf-8", xml_declaration=True)
        with gzip.open(wf_path, "wb") as f:
            f.write(xml_content)
        print(f"âœ… æ½åŠEPGå·²ä¿å­˜ï¼ˆå”¯ä¸€IDï¼šWF-å‰ç¼€ï¼‰")
        return wf_path
    except Exception as e:
        print(f"âŒ æ½åŠæŠ“å–å¤±è´¥: {e}")
        empty = b'<?xml version="1.0" encoding="utf-8"?>\n<tv></tv>'
        p = os.path.join(OUTPUT_DIR, "weifang.gz")
        with gzip.open(p, "wb") as f:
            f.write(empty)
        return p

# ====================== æŠ“å–ä¸Šæ¸¸æºï¼ˆè¿‡æ»¤æ½åŠé¢‘é“ï¼‰ ======================
def fetch_with_retry(u):
    for _ in range(MAX_RETRY):
        try:
            r = requests.get(u, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
            if r.status_code not in (200, 206):
                time.sleep(1)
                continue
            # è§£å‹å¹¶æ¸…ç†æ•°æ®
            c = gzip.decompress(r.content).decode("utf-8","ignore") if u.endswith(".gz") else r.text
            c = re.sub(r'[\x00-\x1f]', '', c).replace("& ", "&amp; ")
            tree = etree.fromstring(c.encode("utf-8"))
            
            # å…³é”®è¿‡æ»¤ï¼šåˆ é™¤ä¸Šæ¸¸æºä¸­ä¸æ½åŠé¢‘é“åŒåçš„channelå’Œprogramme
            # 1. æ”¶é›†éœ€è¦åˆ é™¤çš„ä¸Šæ¸¸æ½åŠé¢‘é“ID
            upstream_wf_channel_ids = set()
            for ch in tree.xpath("//channel"):
                dn = ch.find("display-name[@lang='zh']")
                if dn is not None and dn.text in WEIFANG_DISPLAY_NAMES:
                    upstream_wf_channel_ids.add(ch.get("id", ""))
            
            # 2. åˆ é™¤ä¸Šæ¸¸çš„æ½åŠé¢‘é“å’Œå¯¹åº”èŠ‚ç›®
            for ch_id in upstream_wf_channel_ids:
                for ch in tree.xpath(f"//channel[@id='{ch_id}']"):
                    tree.remove(ch)
                for prog in tree.xpath(f"//programme[@channel='{ch_id}']"):
                    tree.remove(prog)
            
            # éªŒè¯è¿‡æ»¤åçš„æ•°æ®æœ‰æ•ˆæ€§
            ch_count = len(tree.xpath("//channel"))
            pg_count = len(tree.xpath("//programme"))
            if ch_count>0 and pg_count>0:
                return True, tree
        except Exception as e:
            print(f"âš ï¸ æŠ“å–{u}å¤±è´¥: {e}")
            time.sleep(1)
    return False, None

# ====================== åˆå¹¶ï¼ˆæ½åŠä¼˜å…ˆï¼Œå½»åº•é¿å…å†²çªï¼‰ ======================
def merge_all(weifang_gz):
    if not os.path.exists("config.txt"):
        print("âŒ æ— config.txt")
        return

    with open("config.txt", "r", encoding="utf-8") as f:
        urls = [l.strip() for l in f if l.strip().startswith("http")]

    if not urls:
        print("âŒ æ— æœ‰æ•ˆURL")
        return

    print("å¼€å§‹æŠ“å–ä¸Šæ¸¸æºï¼ˆè¿‡æ»¤æ½åŠé¢‘é“ï¼‰...")
    all_trees = []
    with ThreadPoolExecutor(max_workers=6) as exec:
        res = {exec.submit(fetch_with_retry, u):u for u in urls}
        for f in res:
            ok, t = f.result()
            if ok:
                all_trees.append(t)

    # è¯»å–æ½åŠæº
    try:
        with gzip.open(weifang_gz, "rb") as f:
            wf_content = f.read()
            wf_tree = etree.fromstring(wf_content)
            all_trees.append(wf_tree)
    except Exception as e:
        print(f"âš ï¸ æ½åŠæ–‡ä»¶è¯»å–å¤±è´¥: {e}")

    # åˆå¹¶å»é‡ï¼š(é¢‘é“ID, å¼€å§‹æ—¶é—´) ä½œä¸ºå”¯ä¸€é”®ï¼Œåå…¥ä¸ºä¸»ï¼ˆæ½åŠä¼˜å…ˆï¼‰
    final = etree.Element("tv")
    seen_channel_id = set()
    program_map = {}

    for tree in all_trees:
        for node in tree:
            if node.tag == "channel":
                cid = node.get("id", "")
                if cid and cid not in seen_channel_id:
                    seen_channel_id.add(cid)
                    final.append(node)
            elif node.tag == "programme":
                c = node.get("channel", "")
                s = node.get("start", "")
                if c and s:
                    key = (c, s)
                    # åå…¥ä¸ºä¸»ï¼šæ½åŠæºåå¤„ç†ï¼Œè¦†ç›–ä¸Šæ¸¸æºçš„åŒæ—¶é—´èŠ‚ç›®
                    program_map[key] = node

    # æŒ‰é¢‘é“å’Œæ—¶é—´æ’åºè¾“å‡º
    sorted_programs = sorted(program_map.values(), key=lambda x: (x.get("channel", ""), x.get("start", "")))
    for p in sorted_programs:
        final.append(p)

    # ä¿å­˜æœ€ç»ˆæ–‡ä»¶
    out = os.path.join(OUTPUT_DIR, "epg.gz")
    xml = etree.tostring(final, encoding="utf-8", xml_declaration=True)
    with gzip.open(out, "wb") as f:
        f.write(xml)

    size_mb = os.path.getsize(out) / 1024 / 1024
    print(f"âœ… åˆå¹¶å®Œæˆï¼æ–‡ä»¶å¤§å°ï¼š{size_mb:.2f}MB")
    print(f"âœ… é¢‘é“ï¼š{len(seen_channel_id)}  èŠ‚ç›®ï¼š{len(program_map)}")
    print("ğŸ“ è¾“å‡ºï¼š" + out)

# ====================== å…¥å£ ======================
if __name__ == "__main__":
    try:
        wf = crawl_weifang()
        merge_all(wf)
        print("ğŸ‰ å…¨éƒ¨å®Œæˆï¼Œæ½åŠé¢‘é“ä¸å¤®è§†èŠ‚ç›®å·²åˆ†ç¦»ï¼")
    except Exception as e:
        print(f"âŒ å¤±è´¥ï¼š{e}")
        import traceback
        traceback.print_exc()
