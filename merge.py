import os
import gzip
import re
import time
import signal
import requests
from lxml import etree
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor
from collections import defaultdict
import hashlib

# 10åˆ†é’Ÿå¼ºåˆ¶ç»ˆæ­¢
signal.signal(signal.SIGALRM, lambda s, f: os._exit(0))
signal.alarm(600)

OUTPUT_DIR = "output"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ====================== æ½åŠ4é¢‘é“é…ç½® ======================
WEIFANG_CHANNELS = [
    ("æ½åŠæ–°é—»é¢‘é“", "https://m.tvsou.com/epg/db502561"),
    ("æ½åŠç»æµç”Ÿæ´»é¢‘é“", "https://m.tvsou.com/epg/47a9d24a"),
    ("æ½åŠç§‘æ•™é¢‘é“", "https://m.tvsou.com/epg/d131d3d1"),
    ("æ½åŠå…¬å…±é¢‘é“", "https://m.tvsou.com/epg/c06f0cc0")
]

# ç½‘ç«™å›ºå®šåç¼€ï¼šå‘¨ä¸€w1 ~ å‘¨æ—¥w7
WEEK_MAP = {
    "å‘¨ä¸€": "w1",
    "å‘¨äºŒ": "w2",
    "å‘¨ä¸‰": "w3",
    "å‘¨å››": "w4",
    "å‘¨äº”": "w5",
    "å‘¨å…­": "w6",
    "å‘¨æ—¥": "w7"
}

MAX_RETRY = 2

# === å¿…åº”Referer é˜²åçˆ¬ ===
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Linux; Android 12; Mobile) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Mobile Safari/537.36",
    "Referer": "https://www.bing.com/search?q=%E7%94%B5%E8%A7%86%E8%8A%82%E7%9B%AE%E8%A1%A8"
}

# --- å¯é€‰Seleniumï¼ˆä¸è£…ä¹Ÿèƒ½è·‘ï¼‰---
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

# ====================== å·¥å…·å‡½æ•° ======================
def time_to_xmltv(base_date, time_str):
    try:
        hh, mm = time_str.strip().split(":")
        dt = datetime.combine(base_date, datetime.min.time().replace(hour=int(hh), minute=int(mm)))
        return dt.strftime("%Y%m%d%H%M%S +0800")
    except:
        return ""

def get_page_html(url):
    try:
        resp = requests.get(url, headers=HEADERS, timeout=12)
        resp.encoding = 'utf-8'
        html_content = resp.text
        if re.findall(r'\d{1,2}:\d{2}', html_content):
            return html_content
    except Exception:
        pass

    if SELENIUM_AVAILABLE:
        try:
            opt = Options()
            opt.add_argument("--headless")
            opt.add_argument("--no-sandbox")
            opt.add_argument("--disable-dev-shm-usage")
            opt.add_argument(f"user-agent={HEADERS['User-Agent']}")
            driver = webdriver.Chrome(options=opt)
            driver.get(url)
            time.sleep(2.5)
            html_content = driver.page_source
            driver.quit()
            return html_content
        except Exception:
            pass
    return ""

def clean_channel_name(name):
    """æ¸…ç†é¢‘é“åç§°"""
    if not name:
        return ""
    # ç§»é™¤å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
    name = re.sub(r'\s+', ' ', name.strip())
    # æ ‡å‡†åŒ–ä¸€äº›å¸¸è§åç§°
    name = re.sub(r'CCTV-(\d+)', r'CCTV\1', name)
    name = re.sub(r'CCTV(\d+)é«˜æ¸…', r'CCTV\1', name)
    name = re.sub(r'CCTV(\d+)HD', r'CCTV\1', name)
    return name

def create_channel_id(name):
    """ä»é¢‘é“åç§°åˆ›å»ºè§„èŒƒçš„é¢‘é“ID"""
    if not name:
        return "unknown"
    
    # ç§»é™¤æ‰€æœ‰éå­—æ¯æ•°å­—å­—ç¬¦ï¼Œç”¨ä¸‹åˆ’çº¿è¿æ¥
    clean_id = re.sub(r'[^\w]+', '_', name.strip())
    # ç§»é™¤è¿ç»­çš„ä¸‹åˆ’çº¿
    clean_id = re.sub(r'_+', '_', clean_id)
    # ç§»é™¤é¦–å°¾ä¸‹åˆ’çº¿
    clean_id = clean_id.strip('_')
    # ç¡®ä¿ä»¥å­—æ¯å¼€å¤´
    if clean_id and not clean_id[0].isalpha():
        clean_id = 'ch_' + clean_id
    
    return clean_id if clean_id else f"channel_{hashlib.md5(name.encode()).hexdigest()[:8]}"

# ====================== æ ¸å¿ƒï¼šæŠ“ã€æœ¬å‘¨ä¸€ ~ å‘¨æ—¥ã€‘7å¤© ======================
def get_channel_7days(channel_name, base_url):
    week_list = list(WEEK_MAP.items())
    today = datetime.now()
    monday = today - timedelta(days=today.weekday())
    channel_progs = []

    for i, (week_name, w_suffix) in enumerate(week_list):
        current_date = monday + timedelta(days=i)

        if base_url.endswith('/'):
            url = f"{base_url}{w_suffix}"
        else:
            url = f"{base_url}/{w_suffix}"

        html_content = get_page_html(url)
        if not html_content:
            time.sleep(1)
            continue

        soup = BeautifulSoup(html_content, "html.parser")
        items = soup.find_all("div", class_=re.compile("program-item|time-item", re.I))
        if not items:
            items = soup.find_all("li")

        day_progs = []
        for item in items:
            txt = item.get_text(strip=True)
            match = re.search(r'(\d{1,2}:\d{2})\s*(.+)', txt)
            if not match:
                continue
            t_str, title = match.groups()
            if len(title) < 2 or 'å¹¿å‘Š' in title or 'æŠ¥æ—¶' in title:
                continue
            day_progs.append((t_str.strip(), title.strip()))

        day_progs = sorted(list(set(day_progs)), key=lambda x: x[0])
        for idx in range(len(day_progs)):
            t_start, title = day_progs[idx]
            if idx < len(day_progs)-1:
                t_end = day_progs[idx+1][0]
            else:
                h, m = map(int, t_start.split(':'))
                end_dt = datetime(2000, 1, 1, h, m) + timedelta(minutes=30)
                t_end = end_dt.strftime("%H:%M")

            start = time_to_xmltv(current_date, t_start)
            end = time_to_xmltv(current_date, t_end)
            if start and end:
                channel_progs.append((start, end, title))
        time.sleep(1.0)
    return channel_progs

# ====================== æ½åŠ7å¤©æŠ“å– ======================
def crawl_weifang():
    try:
        root = etree.Element("tv")
        for ch_name, _ in WEIFANG_CHANNELS:
            ch_id = create_channel_id(ch_name)
            ch = etree.SubElement(root, "channel", id=ch_id)
            dn = etree.SubElement(ch, "display-name", lang="zh")
            dn.text = ch_name

        for ch_name, base_url in WEIFANG_CHANNELS:
            programs = get_channel_7days(channel_name=ch_name, base_url=base_url)
            ch_id = create_channel_id(ch_name)
            for start, stop, title in programs:
                prog = etree.SubElement(root, "programme", start=start, stop=stop, channel=ch_id)
                t = etree.SubElement(prog, "title", lang="zh")
                t.text = title

        wf_path = os.path.join(OUTPUT_DIR, "weifang.gz")
        xml_content = etree.tostring(root, encoding="utf-8", pretty_print=True, xml_declaration=True)
        
        with gzip.open(wf_path, "wb") as f:
            f.write(xml_content)
            
        print(f"âœ… æ½åŠEPGå·²ä¿å­˜: {wf_path}")
        return wf_path
        
    except Exception as e:
        print(f"âŒ æ½åŠæºæŠ“å–å¤±è´¥: {e}")
        wf_path = os.path.join(OUTPUT_DIR, "weifang.gz")
        empty_xml = b'<?xml version="1.0" encoding="utf-8"?>\n<tv></tv>'
        with gzip.open(wf_path, "wb") as f:
            f.write(empty_xml)
        return wf_path

# ====================== XMLä¿®å¤å’Œæ¸…æ´—å‡½æ•° ======================
def extract_valid_xml(content):
    """ä»å¯èƒ½æ ¼å¼é”™è¯¯çš„å†…å®¹ä¸­æå–æœ‰æ•ˆçš„XML"""
    if not content:
        return None
    
    # 1. æŸ¥æ‰¾æ‰€æœ‰æœ‰æ•ˆçš„channelå…ƒç´ 
    channel_pattern = r'<channel\s+[^>]*id\s*=\s*["\'][^"\']+["\'][^>]*>.*?</channel>'
    channels = re.findall(channel_pattern, content, re.DOTALL | re.IGNORECASE)
    
    # 2. æŸ¥æ‰¾æ‰€æœ‰æœ‰æ•ˆçš„programmeå…ƒç´ 
    programme_pattern = r'<programme\s+[^>]*start\s*=\s*["\'][^"\']+["\'][^>]*stop\s*=\s*["\'][^"\']+["\'][^>]*>.*?</programme>'
    programmes = re.findall(programme_pattern, content, re.DOTALL | re.IGNORECASE)
    
    # å¦‚æœæ‰¾åˆ°äº†å†…å®¹ï¼Œé‡æ–°æ„å»ºè§„èŒƒçš„XML
    if channels or programmes:
        xml_parts = ['<?xml version="1.0" encoding="utf-8"?>', '<tv>']
        xml_parts.extend(channels)
        xml_parts.extend(programmes)
        xml_parts.append('</tv>')
        return '\n'.join(xml_parts)
    
    return None

def fetch_with_retry(u, max_retry=MAX_RETRY):
    for attempt in range(1, max_retry + 1):
        try:
            r = requests.get(u, timeout=15, headers={"User-Agent": "Mozilla/5.0"})
            if r.status_code not in (200, 206):
                time.sleep(1)
                continue

            if u.endswith(".gz"):
                content = gzip.decompress(r.content).decode("utf-8", "ignore")
            else:
                content = r.text

            # å°è¯•ç›´æ¥è§£æ
            try:
                parser = etree.XMLParser(recover=True)
                tree = etree.fromstring(content.encode("utf-8"), parser=parser)
                ch = len(tree.xpath("//channel"))
                pg = len(tree.xpath("//programme"))
                if ch > 0 and pg > 0:
                    return (True, tree, ch, pg, attempt)
            except:
                pass
            
            # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•æå–æœ‰æ•ˆå†…å®¹
            fixed_xml = extract_valid_xml(content)
            if fixed_xml:
                parser = etree.XMLParser(recover=True)
                tree = etree.fromstring(fixed_xml.encode("utf-8"), parser=parser)
                ch = len(tree.xpath("//channel"))
                pg = len(tree.xpath("//programme"))
                if ch > 0 and pg > 0:
                    return (True, tree, ch, pg, attempt)
                    
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥ {u[:50]}...: {e}")
            time.sleep(1)
            continue
    return (False, None, 0, 0, max_retry)

def merge_all(weifang_gz_file):
    # å­˜å‚¨å¤„ç†åçš„é¢‘é“å’ŒèŠ‚ç›®
    channel_data = {}  # é¢‘é“ID -> (æ˜¾ç¤ºåç§°, åŸå§‹é¢‘é“èŠ‚ç‚¹)
    program_data = defaultdict(list)  # é¢‘é“ID -> èŠ‚ç›®åˆ—è¡¨
    
    total_ch = 0
    total_pg = 0
    success_cnt = 0
    fail_cnt = 0

    if not os.path.exists("config.txt"):
        print("âŒ æœªæ‰¾åˆ° config.txt æ–‡ä»¶")
        return

    with open("config.txt", "r", encoding="utf-8") as f:
        urls = [l.strip() for l in f if l.strip() and l.startswith("http")]

    if not urls:
        print("âŒ config.txt ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„URL")
        return

    print("=" * 60)
    print("EPG æºæŠ“å–ç»Ÿè®¡ï¼ˆå¤±è´¥è‡ªåŠ¨é‡è¯•ï¼‰")
    print("=" * 60)

    with ThreadPoolExecutor(max_workers=3) as executor:
        future_map = {executor.submit(fetch_with_retry, u): u for u in urls}
        for fut in future_map:
            u = future_map[fut]
            ok, tree, ch, pg, retry_cnt = fut.result()
            if ok:
                success_cnt += 1
                total_ch += ch
                total_pg += pg
                log_retry = f"[é‡è¯•{retry_cnt-1}æ¬¡]" if retry_cnt > 1 else ""
                print(f"âœ… {u[:55]}... {log_retry}æˆåŠŸ | é¢‘é“ {ch:>4} | èŠ‚ç›® {pg:>6}")
                
                # å¤„ç†é¢‘é“
                for channel in tree.xpath("//channel"):
                    try:
                        channel_id = channel.get('id')
                        if not channel_id:
                            continue
                            
                        display_name = channel.findtext("display-name", "").strip()
                        if not display_name:
                            display_name = channel_id
                        
                        # æ¸…ç†é¢‘é“åç§°
                        clean_name = clean_channel_name(display_name)
                        clean_id = create_channel_id(clean_name)
                        
                        # å­˜å‚¨é¢‘é“æ•°æ®
                        channel_data[clean_id] = (clean_name, channel)
                        
                    except Exception as e:
                        print(f"âš ï¸ å¤„ç†é¢‘é“æ—¶å‡ºé”™: {e}")
                        continue
                
                # å¤„ç†èŠ‚ç›®
                for programme in tree.xpath("//programme"):
                    try:
                        channel_id = programme.get('channel')
                        start = programme.get('start')
                        stop = programme.get('stop')
                        title = programme.findtext("title", "").strip()
                        
                        if not all([channel_id, start, stop, title]):
                            continue
                            
                        # æŸ¥æ‰¾å¯¹åº”çš„é¢‘é“ID
                        display_name = ""
                        for ch_id, (ch_name, _) in channel_data.items():
                            # å¦‚æœé¢‘é“IDåŒ¹é…æˆ–æ˜¾ç¤ºåç§°åŒ¹é…
                            if channel_id == ch_id:
                                clean_id = ch_id
                                break
                        else:
                            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…ï¼Œå°è¯•æ¸…ç†åŸå§‹é¢‘é“ID
                            clean_id = create_channel_id(channel_id)
                        
                        # æ£€æŸ¥èŠ‚ç›®æ˜¯å¦é‡å¤ï¼ˆç›¸åŒçš„é¢‘é“ã€å¼€å§‹æ—¶é—´å’Œæ ‡é¢˜ï¼‰
                        program_key = f"{clean_id}_{start}_{hashlib.md5(title.encode()).hexdigest()[:8]}"
                        program_data[clean_id].append((start, stop, title, program_key))
                        
                    except Exception as e:
                        print(f"âš ï¸ å¤„ç†èŠ‚ç›®æ—¶å‡ºé”™: {e}")
                        continue
            else:
                fail_cnt += 1

    if fail_cnt > 0:
        print(f"âŒ å…± {fail_cnt} ä¸ªæºç»{MAX_RETRY}æ¬¡é‡è¯•åä»å¤±è´¥ï¼Œå·²è·³è¿‡")

    print("=" * 60)
    print(f"æ±‡æ€»ï¼šæˆåŠŸ {success_cnt} ä¸ª | å¤±è´¥ {fail_cnt} ä¸ª | æ€»é¢‘é“ {len(channel_data)} | æ€»èŠ‚ç›® {sum(len(v) for v in program_data.values())}")
    print("=" * 60)

    # æ·»åŠ æ½åŠæœ¬åœ°æº
    try:
        with gzip.open(weifang_gz_file, "rb") as f:
            wf_content = f.read().decode("utf-8")
            parser = etree.XMLParser(recover=True)
            wf_tree = etree.fromstring(wf_content.encode("utf-8"), parser)
            
            for channel in wf_tree.xpath("//channel"):
                channel_id = channel.get('id')
                display_name = channel.findtext("display-name", "").strip()
                
                if channel_id and display_name:
                    clean_name = clean_channel_name(display_name)
                    clean_id = create_channel_id(clean_name)
                    channel_data[clean_id] = (clean_name, channel)
            
            for programme in wf_tree.xpath("//programme"):
                channel_id = programme.get('channel')
                start = programme.get('start')
                stop = programme.get('stop')
                title = programme.findtext("title", "").strip()
                
                if not all([channel_id, start, stop, title]):
                    continue
                
                # æŸ¥æ‰¾å¯¹åº”çš„é¢‘é“ID
                display_name = ""
                for ch_id, (ch_name, _) in channel_data.items():
                    if channel_id == ch_id:
                        clean_id = ch_id
                        break
                else:
                    clean_id = create_channel_id(channel_id)
                
                program_key = f"{clean_id}_{start}_{hashlib.md5(title.encode()).hexdigest()[:8]}"
                program_data[clean_id].append((start, stop, title, program_key))
                
    except Exception as e:
        print(f"âš ï¸ æ½åŠæœ¬åœ°æºè¯»å–å¤±è´¥: {e}")

    print(f"å¤„ç†åçš„é¢‘é“æ•°é‡: {len(channel_data)}")
    print(f"å¤„ç†åçš„èŠ‚ç›®æ•°é‡: {sum(len(v) for v in program_data.values())}")
    
    # ====================== å»é‡èŠ‚ç›® ======================
    print("å¼€å§‹å»é‡èŠ‚ç›®...")
    for channel_id in list(program_data.keys()):
        programs = program_data[channel_id]
        # ä½¿ç”¨é›†åˆå»é‡
        unique_programs = {}
        for start, stop, title, key in programs:
            unique_programs[key] = (start, stop, title)
        # æŒ‰å¼€å§‹æ—¶é—´æ’åº
        sorted_programs = sorted(unique_programs.values(), key=lambda x: x[0])
        program_data[channel_id] = sorted_programs
    
    total_unique_programs = sum(len(v) for v in program_data.values())
    print(f"å»é‡åçš„èŠ‚ç›®æ•°é‡: {total_unique_programs}")
    
    # ====================== ç”Ÿæˆæœ€ç»ˆXML ======================
    print("ç”Ÿæˆæœ€ç»ˆXML...")
    root = etree.Element("tv")
    
    # æ·»åŠ é¢‘é“
    for channel_id, (display_name, _) in sorted(channel_data.items()):
        ch = etree.SubElement(root, "channel", id=channel_id)
        dn = etree.SubElement(ch, "display-name", lang="zh")
        dn.text = display_name
    
    # æ·»åŠ èŠ‚ç›®
    for channel_id, programs in program_data.items():
        for start, stop, title in programs:
            prog = etree.SubElement(root, "programme", start=start, stop=stop, channel=channel_id)
            t = etree.SubElement(prog, "title", lang="zh")
            t.text = title
    
    # ç”ŸæˆXML
    xml_declaration = '<?xml version="1.0" encoding="utf-8"?>\n'
    xml_str = xml_declaration.encode('utf-8') + etree.tostring(root, encoding="utf-8", pretty_print=True)
    
    # ä¿å­˜å‹ç¼©æ–‡ä»¶
    output_path = os.path.join(OUTPUT_DIR, "epg.gz")
    with gzip.open(output_path, "wb") as f:
        f.write(xml_str)
    
    # è®¡ç®—æ–‡ä»¶å¤§å°
    file_size_mb = os.path.getsize(output_path) / 1024 / 1024
    print(f"âœ… æœ€ç»ˆè¾“å‡ºï¼šé¢‘é“ {len(channel_data)} ä¸ª | èŠ‚ç›® {total_unique_programs} ä¸ª")
    print(f"ğŸ“¦ æ–‡ä»¶å¤§å°ï¼š{file_size_mb:.2f} MB")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶ï¼š{output_path}")
    
    # ä¿å­˜æœªå‹ç¼©çš„XMLç”¨äºè°ƒè¯•
    xml_debug_path = os.path.join(OUTPUT_DIR, "epg.xml")
    with open(xml_debug_path, "wb") as f:
        f.write(xml_str)
    print(f"ğŸ“ è°ƒè¯•æ–‡ä»¶ï¼ˆæœªå‹ç¼©ï¼‰ï¼š{xml_debug_path}")
    
    # æ˜¾ç¤ºå‹ç¼©å‰åå¤§å°å¯¹æ¯”
    if os.path.exists(os.path.join(OUTPUT_DIR, "weifang.gz")):
        wf_size = os.path.getsize(os.path.join(OUTPUT_DIR, "weifang.gz")) / 1024
        epg_size = os.path.getsize(output_path) / 1024
        print(f"ğŸ“Š å¤§å°å¯¹æ¯”ï¼šæ½åŠæº {wf_size:.1f} KB | åˆå¹¶å {epg_size:.1f} KB | å·®å¼‚ {epg_size-wf_size:.1f} KB")
    
    print("=" * 60)

# ====================== å…¥å£ ======================
if __name__ == "__main__":
    try:
        print("å¼€å§‹æŠ“å–æ½åŠæœ¬åœ°EPG...")
        wf_gz = crawl_weifang()
        print("å¼€å§‹åˆå¹¶æ‰€æœ‰EPGæº...")
        merge_all(wf_gz)
        print("âœ… EPGåˆå¹¶å®Œæˆï¼")
    except Exception as e:
        print(f"âŒ è„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
