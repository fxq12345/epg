import os
import gzip
import re
import time
import signal
import requests
from lxml import etree
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor

# 10åˆ†é’Ÿå¼ºåˆ¶ç»ˆæ­¢
signal.signal(signal.SIGALRM, lambda s, f: os._exit(0))
signal.alarm(600)

OUTPUT_DIR = "output"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ====================== ä½ çš„åŸå§‹é¢‘é“åˆ—è¡¨ ======================
WEIFANG_CHANNELS = [
    (
        "æ½åŠæ–°é—»é¢‘é“", 
        "https://m.tvsou.com/epg/db502561",
        "https://picsum.photos/seed/weifang-news/200/120"
    ),
    (
        "æ½åŠç»æµç”Ÿæ´»é¢‘é“", 
        "https://m.tvsou.com/epg/47a9d24a",
        "https://picsum.photos/seed/weifang-econ/200/120"
    ),
    (
        "æ½åŠç§‘æ•™é¢‘é“", 
        "https://m.tvsou.com/epg/d131d3d1",
        "https://picsum.photos/seed/weifang-sci/200/120"
    ),
    (
        "æ½åŠå…¬å…±é¢‘é“", 
        "https://m.tvsou.com/epg/c06f0cc0",
        "https://picsum.photos/seed/weifang-public/200/120"
    )
]

WEEK_DAY = ["w1", "w2", "w3", "w4", "w5", "w6", "w7"]
MAX_RETRY = 2

# ====================== ä¿®æ”¹åçš„æŠ“å–é€»è¾‘ï¼ˆç²¾å‡†æ—¶é—´+é˜²æ‹¦æˆªï¼‰ ======================

# --- æ–°å¢ï¼šå¢å¼ºçš„è¯·æ±‚å¤´ ---
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    # æ¨¡æ‹Ÿä»æœç´¢å¼•æ“ç‚¹å‡»è¿›å…¥ï¼Œè§£å†³é˜²ç›—é“¾
    "Referer": "https://www.baidu.com/s?wd=æ½åŠç”µè§†å°èŠ‚ç›®è¡¨" 
}

def crawl_weifang_single(ch_name, base_url, day_str, current_day):
    # åŸºäºä½ çš„åŸå§‹é€»è¾‘ï¼Œä½†å¢åŠ äº†è¯·æ±‚å¤´
    for attempt in range(1, MAX_RETRY + 1):
        try:
            url = f"{base_url}/{day_str}"
            print(f"å°è¯•æŠ“å– {ch_name} ({day_str}): {url}")
            
            # å‘é€è¯·æ±‚
            resp = requests.get(url, headers=HEADERS, timeout=10)
            resp.encoding = "utf-8"
            
            # æ£€æŸ¥å“åº”çŠ¶æ€
            if resp.status_code != 200:
                print(f"çŠ¶æ€ç é”™è¯¯: {resp.status_code}")
                time.sleep(1)
                continue
                
            html = resp.text
            
            # ç®€å•çš„åçˆ¬è™«æ£€æŸ¥
            if "è®¿é—®è¿‡äºé¢‘ç¹" in html or "è¯·è¾“å…¥éªŒè¯ç " in html:
                print(f"è­¦å‘Š: {url} è§¦å‘åçˆ¬è™«ï¼Œå°è¯•é‡è¯•")
                time.sleep(3)
                continue
                
            soup = BeautifulSoup(html, "html.parser")
            
            program_list = []
            # æŸ¥æ‰¾åŒ…å«æ—¶é—´çš„å…ƒç´ ï¼Œå…¼å®¹å¤šç§æ ‡ç­¾
            # tvsou çš„ç»“æ„é€šå¸¸æ˜¯ li æˆ– div åŒ…å«æ—¶é—´
            items = soup.find_all(["li", "div", "p"])
            
            for item in items:
                txt = item.get_text(strip=True)
                # æ­£åˆ™åŒ¹é…æ—¶é—´æ ¼å¼ï¼Œå¦‚ "08:00 èŠ‚ç›®å"
                match = re.match(r"(\d{1,2}:\d{2})\s*(.+)", txt)
                if not match:
                    continue
                time_str, title = match.groups()
                
                # è¿‡æ»¤æ— æ•ˆæ•°æ®
                if len(title) < 2 or "å¹¿å‘Š" in title or "æµ‹è¯•å¡" in title:
                    continue
                    
                # æ„å»ºå‡†ç¡®çš„æ—¶é—´å¯¹è±¡
                try:
                    hh, mm = map(int, time_str.split(":"))
                    prog_time = datetime.combine(current_day, datetime.min.time().replace(hour=hh, minute=mm))
                    program_list.append((prog_time, title))
                except ValueError:
                    continue
            
            # å¦‚æœæ²¡æŠ“åˆ°æ•°æ®ï¼Œè·³è¿‡
            if not program_list:
                print(f"è­¦å‘Š: {url} æœªæ‰¾åˆ°æœ‰æ•ˆèŠ‚ç›®æ•°æ®")
                continue
                
            # ç”Ÿæˆç²¾å‡†çš„å¼€å§‹å’Œç»“æŸæ—¶é—´
            precise_programs = []
            for i in range(len(program_list)):
                start_time, title = program_list[i]
                if i == len(program_list) - 1:
                    # æœ€åä¸€ä¸ªèŠ‚ç›®ï¼Œå‡è®¾æ—¶é•¿60åˆ†é’Ÿ
                    stop_time = start_time + timedelta(minutes=60)
                else:
                    stop_time = program_list[i+1][0]
                
                start_xml = start_time.strftime("%Y%m%d%H%M%S +0800")
                stop_xml = stop_time.strftime("%Y%m%d%H%M%S +0800")
                precise_programs.append((start_xml, stop_xml, title))
            
            time.sleep(0.5) # å‡å°‘å¹¶å‘å‹åŠ›
            return precise_programs
            
        except Exception as e:
            print(f"æŠ“å–å¼‚å¸¸: {e}")
            time.sleep(1)
            continue
    return []

# ====================== ä¿®æ”¹åçš„æ—¶é—´è®¡ç®—é€»è¾‘ ======================

def crawl_weifang():
    try:
        root = etree.Element("tv")
        
        # 1. å…ˆç”Ÿæˆé¢‘é“èŠ‚ç‚¹
        for ch_name, base_url, icon_url in WEIFANG_CHANNELS:
            ch = etree.SubElement(root, "channel", id=ch_name)
            dn = etree.SubElement(ch, "display-name")
            dn.text = ch_name
            icon = etree.SubElement(ch, "icon", src=icon_url)

        # --- å…³é”®ä¿®æ”¹ï¼šè®¡ç®—æœ¬å‘¨ä¸€ä½œä¸ºåŸºå‡† ---
        # è·å–å½“å‰æ—¶é—´
        now = datetime.now()
        # è®¡ç®—æœ¬å‘¨ä¸€çš„æ—¥æœŸ (weekday() è¿”å› 0-6, Monday is 0)
        # ä½¿ç”¨ isoweekday() è¿”å› 1-7, Monday is 1
        weekday = now.isoweekday() # 1=å‘¨ä¸€, 7=å‘¨æ—¥
        # è®¡ç®—åç§»é‡ï¼Œå°†ä»Šå¤©è°ƒæ•´åˆ°æœ¬å‘¨ä¸€
        offset = weekday - 1
        # å¾—åˆ°æœ¬å‘¨ä¸€çš„æ—¥æœŸå¯¹è±¡
        monday = now - timedelta(days=offset)
        # å°†æ—¶é—´å½’é›¶ (æ—¶åˆ†ç§’è®¾ä¸º00:00:00)
        monday = monday.replace(hour=0, minute=0, second=0, microsecond=0)
        
        # 2. å¾ªç¯æŠ“å–å‘¨ä¸€åˆ°å‘¨æ—¥ (w1 åˆ° w7)
        for day_idx in range(7):
            # è®¡ç®—å½“å‰å¾ªç¯å¯¹åº”çš„æ—¥æœŸ (å‘¨ä¸€ + å¤©æ•°åç§»)
            current_day = monday + timedelta(days=day_idx)
            day_str = WEEK_DAY[day_idx] # w1, w2, ... w7
            
            for ch_name, base_url, _ in WEIFANG_CHANNELS:
                programs = crawl_weifang_single(ch_name, base_url, day_str, current_day)
                for start, stop, title in programs:
                    prog = etree.SubElement(root, "programme", start=start, stop=stop, channel=ch_name)
                    t = etree.SubElement(prog, "title")
                    t.text = title

        # ä»…ç”Ÿæˆ gzï¼Œä¸ç”Ÿæˆ xml
        wf_path = os.path.join(OUTPUT_DIR, "weifang.gz")
        xml_content = etree.tostring(root, encoding="utf-8", pretty_print=True)
        with gzip.open(wf_path, "wb") as f:
            f.write(xml_content)
        return wf_path
        
    except Exception as e:
        print(f"ä¸»æŠ“å–æµç¨‹é”™è¯¯: {e}")
        # å¤±è´¥ä¹Ÿå†™å…¥ç©ºgz
        wf_path = os.path.join(OUTPUT_DIR, "weifang.gz")
        empty_xml = b'<?xml version="1.0" encoding="utf-8"?>\n<tv></tv>'
        with gzip.open(wf_path, "wb") as f:
            f.write(empty_xml)
        return wf_path

# ====================== ä½ åŸæœ‰çš„å…¶ä»–å‡½æ•°ä¿æŒä¸å˜ ======================

def fetch_with_retry(u, max_retry=MAX_RETRY):
    for attempt in range(1, max_retry + 1):
        try:
            r = requests.get(u, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
            if r.status_code not in (200, 206):
                time.sleep(1)
                continue

            if u.endswith(".gz"):
                content = gzip.decompress(r.content).decode("utf-8", "ignore")
            else:
                content = r.text

            content = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f]', '', content).replace("& ", "&amp; ")
            tree = etree.fromstring(content.encode("utf-8"))
            ch = len(tree.xpath("//channel"))
            pg = len(tree.xpath("//programme"))
            if ch > 0 and pg > 0:
                return (True, tree, ch, pg, attempt)
        except:
            time.sleep(1)
            continue
    return (False, None, 0, 0, max_retry)

def merge_all(weifang_gz_file):
    all_channels = []
    all_programs = []
    total_ch = 0
    total_pg = 0
    success_cnt = 0
    fail_cnt = 0

    if not os.path.exists("config.txt"):
        return

    with open("config.txt", "r", encoding="utf-8") as f:
        urls = [l.strip() for l in f if l.strip() and l.startswith("http")]

    print("=" * 60)
    print("EPG æºæŠ“å–ç»Ÿè®¡ï¼ˆå¤±è´¥è‡ªåŠ¨é‡è¯•ï¼‰")
    print("=" * 60)

    with ThreadPoolExecutor(max_workers=6) as executor:
        future_map = {executor.submit(fetch_with_retry, u): u for u in urls}
        for fut in future_map:
            u = future_map[fut]
            ok, tree, ch, pg, retry_cnt = fut.result()
            if ok:
                success_cnt += 1
                total_ch += ch
                total_pg += pg
                log_retry = f"[é‡è¯•{retry_cnt-1}æ¬¡]" if retry_cnt > 1 else ""
                print(f"âœ… {u[:55]}... {log_retry}æˆåŠŸ | é¢‘é“ {ch:>4} | èŠ‚ç›® {pg:>6}")
                for node in tree:
                    if node.tag == "channel":
                        all_channels.append(node)
                    elif node.tag == "programme":
                        all_programs.append(node)
            else:
                fail_cnt += 1

    if fail_cnt > 0:
        print(f"âŒ å…± {fail_cnt} ä¸ªæºç»{MAX_RETRY}æ¬¡é‡è¯•åä»å¤±è´¥ï¼Œå·²è·³è¿‡")

    print("=" * 60)
    print(f"æ±‡æ€»ï¼šæˆåŠŸ {success_cnt} ä¸ª | å¤±è´¥ {fail_cnt} ä¸ª | æ€»é¢‘é“ {total_ch} | æ€»èŠ‚ç›® {total_pg}")
    print("=" * 60)

    # ====================== è¯»å–æ½åŠ gz æ–‡ä»¶åˆå¹¶ ======================
    try:
        with gzip.open(weifang_gz_file, "rb") as f:
            wf_content = f.read().decode("utf-8")
            wf_tree = etree.fromstring(wf_content.encode("utf-8"))
            wf_ch = len(wf_tree.xpath("//channel"))
            wf_pg = len(wf_tree.xpath("//programme"))

        if wf_ch > 0 and wf_pg > 0:
            print(f"ğŸ“º æ½åŠæœ¬åœ°æºï¼šé¢‘é“ {wf_ch} | èŠ‚ç›® {wf_pg}ï¼ˆæ—¶é—´ç²¾å‡†åŒ¹é…+é…·9å›¾æ ‡ï¼‰")
            for node in wf_tree:
                if node.tag == "channel":
                    all_channels.append(node)
                elif node.tag == "programme":
                    all_programs.append(node)
        else:
            print("âš ï¸ æ½åŠæœ¬åœ°æºæŠ“å–å¤±è´¥ï¼Œå·²è·³è¿‡")
    except:
        print("âš ï¸ æ½åŠæœ¬åœ°æºè¯»å–å¤±è´¥ï¼Œå·²è·³è¿‡")

    # æœ€ç»ˆåªç”Ÿæˆ epg.gzï¼Œåˆ é™¤æ˜æ–‡xmlè¾“å‡º
    final_root = etree.Element("tv")
    for ch in all_channels:
        final_root.append(ch)
    for p in all_programs:
        final_root.append(p)

    xml_str = etree.tostring(final_root, encoding="utf-8", pretty_print=True)
    # ä»…è¾“å‡ºå‹ç¼©åŒ…ï¼Œæ— xmlæ–‡ä»¶
    with gzip.open(os.path.join(OUTPUT_DIR, "epg.gz"), "wb") as f:
        f.write(xml_str)

# ====================== å…¥å£ ======================
if __name__ == "__main__":
    try:
        wf_gz = crawl_weifang()
        merge_all(wf_gz)
    except Exception as e:
        print(f"ç¨‹åºè¿è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {e}")
