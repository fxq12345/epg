import os
import gzip
import re
import time
import signal
import requests
from lxml import etree
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor
import html

# 10åˆ†é’Ÿå¼ºåˆ¶ç»ˆæ­¢
signal.signal(signal.SIGALRM, lambda s, f: os._exit(0))
signal.alarm(600)

OUTPUT_DIR = "output"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ====================== æ½åŠ4é¢‘é“é…ç½® ======================
WEIFANG_CHANNELS = [
    ("æ½åŠæ–°é—»é¢‘é“", "https://m.tvsou.com/epg/db502561"),
    ("æ½åŠç»æµç”Ÿæ´»é¢‘é“", "https://m.tvsou.com/epg/47a9d24a"),
    ("æ½åŠç§‘æ•™é¢‘é“", "https://m.tvsou.com/epg/d131d3d1"),
    ("æ½åŠå…¬å…±é¢‘é“", "https://m.tvsou.com/epg/c06f0cc0")
]

# ç½‘ç«™å›ºå®šåç¼€ï¼šå‘¨ä¸€w1 ~ å‘¨æ—¥w7
WEEK_MAP = {
    "å‘¨ä¸€": "w1",
    "å‘¨äºŒ": "w2",
    "å‘¨ä¸‰": "w3",
    "å‘¨å››": "w4",
    "å‘¨äº”": "w5",
    "å‘¨å…­": "w6",
    "å‘¨æ—¥": "w7"
}

MAX_RETRY = 2

# === å¿…åº”Referer é˜²åçˆ¬ ===
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Linux; Android 12; Mobile) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Mobile Safari/537.36",
    "Referer": "https://www.bing.com/search?q=%E7%94%B5%E8%A7%86%E8%8A%82%E7%9B%AE%E8%A1%A8"
}

# --- å¯é€‰Seleniumï¼ˆä¸è£…ä¹Ÿèƒ½è·‘ï¼‰---
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

# ====================== XMLæ¸…æ´—å’Œä¿®å¤å‡½æ•° ======================
def clean_xml_content(content):
    """æ¸…æ´—å’Œä¿®å¤XMLå†…å®¹ï¼Œç§»é™¤æ ¼å¼é”™è¯¯"""
    if not content:
        return ""
    
    # 1. ç§»é™¤æ§åˆ¶å­—ç¬¦ï¼Œä½†ä¿ç•™æ¢è¡Œå’Œåˆ¶è¡¨ç¬¦
    content = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', content)
    
    # 2. ä¿®å¤æœªè½¬ä¹‰çš„&ç¬¦å·ï¼ˆä½†ä¿ç•™å·²è½¬ä¹‰çš„å®ä½“ï¼‰
    content = re.sub(r'&(?!(amp|lt|gt|quot|apos|#x?[0-9a-f]+);)', '&amp;', content)
    
    # 3. ä¿®å¤æ ‡ç­¾åµŒå¥—é”™è¯¯ï¼šç§»é™¤å¤šä½™çš„</channel>é—­åˆæ ‡ç­¾
    # åŒ¹é…æ­£ç¡®çš„channelæ ‡ç­¾å¯¹ï¼Œç„¶åç§»é™¤ä¸åœ¨æ­£ç¡®ä½ç½®çš„</channel>
    lines = content.split('\n')
    cleaned_lines = []
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        # å¦‚æœå½“å‰è¡Œæ˜¯<channelå¼€å¤´ï¼Œåˆ™æ‰¾åˆ°å¯¹åº”çš„ç»“æŸæ ‡ç­¾
        if line.startswith('<channel') and not line.startswith('</channel'):
            # æ”¶é›†è¿™ä¸ªchannelçš„æ‰€æœ‰è¡Œï¼Œç›´åˆ°æ‰¾åˆ°æ­£ç¡®çš„</channel>
            channel_lines = [lines[i]]
            i += 1
            while i < len(lines) and not lines[i].strip().startswith('</channel'):
                channel_lines.append(lines[i])
                i += 1
            if i < len(lines) and lines[i].strip().startswith('</channel'):
                channel_lines.append(lines[i])  # æ·»åŠ æ­£ç¡®çš„é—­åˆæ ‡ç­¾
                i += 1
            # å°†è¿™ä¸ªchannelçš„å†…å®¹åŠ å…¥æ¸…ç†ååˆ—è¡¨
            cleaned_lines.extend(channel_lines)
            continue
        # å¦‚æœé‡åˆ°å­¤ç«‹çš„</channel>ï¼Œè·³è¿‡å®ƒ
        if line == '</channel>':
            i += 1
            continue
        cleaned_lines.append(lines[i])
        i += 1
    
    content = '\n'.join(cleaned_lines)
    
    # 4. ä¿®å¤programmeæ ‡ç­¾å†…çš„é”™è¯¯ï¼šç§»é™¤å¤šä½™çš„<title>å’Œ</channel>
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ¯ä¸ªprogrammeæ ‡ç­¾
    programme_pattern = r'(<programme[^>]*>)(.*?)(</programme>)'
    
    def fix_programme(match):
        opening = match.group(1)
        inner = match.group(2)
        closing = match.group(3)
        
        # ç§»é™¤innerä¸­å¤šä½™çš„</channel>æ ‡ç­¾
        inner = re.sub(r'</channel>', '', inner)
        
        # æå–æ‰€æœ‰titleæ ‡ç­¾ï¼Œåªä¿ç•™æœ€åä¸€ä¸ªï¼ˆå¦‚æœå¤šä¸ªï¼‰
        titles = re.findall(r'<title[^>]*>(.*?)</title>', inner, re.DOTALL)
        if titles:
            # åªä¿ç•™æœ€åä¸€ä¸ªtitle
            last_title = titles[-1].strip()
            # ç§»é™¤innerä¸­æ‰€æœ‰çš„titleæ ‡ç­¾
            inner = re.sub(r'<title[^>]*>.*?</title>', '', inner, flags=re.DOTALL)
            # åœ¨inneræœ«å°¾æ·»åŠ æ­£ç¡®çš„titleæ ‡ç­¾
            inner += f'<title lang="zh">{last_title}</title>'
        
        return opening + inner + closing
    
    content = re.sub(programme_pattern, fix_programme, content, flags=re.DOTALL)
    
    # 5. ä¿®å¤å±æ€§å€¼ä¸­çš„æ¢è¡Œï¼ˆå¦‚å›¾ç‰‡ä¸­start=å’Œstop=åçš„æ¢è¡Œï¼‰
    content = re.sub(r'(\w+)=\s*\n\s*"([^"]+)"', r'\1="\2"', content)
    
    # 6. ç¡®ä¿æ‰€æœ‰æ ‡ç­¾æ­£ç¡®é—­åˆï¼ˆç®€åŒ–å¤„ç†ï¼Œç§»é™¤æ²¡æœ‰å¯¹åº”å¼€å¤´çš„é—­åˆæ ‡ç­¾ï¼‰
    # ç§»é™¤æ²¡æœ‰å¯¹åº”<channel>çš„</channel>
    channel_open = 0
    lines = content.split('\n')
    cleaned_lines = []
    for line in lines:
        stripped = line.strip()
        if stripped.startswith('<channel') and not stripped.startswith('</channel'):
            channel_open += 1
            cleaned_lines.append(line)
        elif stripped == '</channel>':
            if channel_open > 0:
                channel_open -= 1
                cleaned_lines.append(line)
            # å¦åˆ™è·³è¿‡è¿™ä¸ªå¤šä½™çš„é—­åˆæ ‡ç­¾
        else:
            cleaned_lines.append(line)
    content = '\n'.join(cleaned_lines)
    
    return content

# ====================== å·¥å…·å‡½æ•° ======================
def time_to_xmltv(base_date, time_str):
    try:
        hh, mm = time_str.strip().split(":")
        dt = datetime.combine(base_date, datetime.min.time().replace(hour=int(hh), minute=int(mm)))
        return dt.strftime("%Y%m%d%H%M%S +0800")
    except:
        return ""

def get_page_html(url):
    try:
        resp = requests.get(url, headers=HEADERS, timeout=12)
        resp.encoding = 'utf-8'
        html_content = resp.text
        if re.findall(r'\d{1,2}:\d{2}', html_content):
            return html_content
    except Exception:
        pass

    if SELENIUM_AVAILABLE:
        try:
            opt = Options()
            opt.add_argument("--headless")
            opt.add_argument("--no-sandbox")
            opt.add_argument("--disable-dev-shm-usage")
            opt.add_argument(f"user-agent={HEADERS['User-Agent']}")
            driver = webdriver.Chrome(options=opt)
            driver.get(url)
            time.sleep(2.5)
            html_content = driver.page_source
            driver.quit()
            return html_content
        except Exception:
            pass
    return ""

# ====================== æ ¸å¿ƒï¼šæŠ“ã€æœ¬å‘¨ä¸€ ~ æœ¬å‘¨æ—¥ã€‘7å¤© ======================
def get_channel_7days(channel_name, base_url):
    week_list = list(WEEK_MAP.items())
    today = datetime.now()
    monday = today - timedelta(days=today.weekday())
    channel_progs = []

    for i, (week_name, w_suffix) in enumerate(week_list):
        current_date = monday + timedelta(days=i)

        if base_url.endswith('/'):
            url = f"{base_url}{w_suffix}"
        else:
            url = f"{base_url}/{w_suffix}"

        html_content = get_page_html(url)
        if not html_content:
            time.sleep(1)
            continue

        soup = BeautifulSoup(html_content, "html.parser")
        items = soup.find_all("div", class_=re.compile("program-item|time-item", re.I))
        if not items:
            items = soup.find_all("li")

        day_progs = []
        for item in items:
            txt = item.get_text(strip=True)
            match = re.search(r'(\d{1,2}:\d{2})\s*(.+)', txt)
            if not match:
                continue
            t_str, title = match.groups()
            if len(title) < 2 or 'å¹¿å‘Š' in title or 'æŠ¥æ—¶' in title:
                continue
            day_progs.append((t_str.strip(), title.strip()))

        day_progs = sorted(list(set(day_progs)), key=lambda x: x[0])
        for idx in range(len(day_progs)):
            t_start, title = day_progs[idx]
            if idx < len(day_progs)-1:
                t_end = day_progs[idx+1][0]
            else:
                h, m = map(int, t_start.split(':'))
                end_dt = datetime(2000, 1, 1, h, m) + timedelta(minutes=30)
                t_end = end_dt.strftime("%H:%M")

            start = time_to_xmltv(current_date, t_start)
            end = time_to_xmltv(current_date, t_end)
            if start and end:
                channel_progs.append((start, end, title))
        time.sleep(1.0)
    return channel_progs

# ====================== æ½åŠ7å¤©æŠ“å– ======================
def crawl_weifang():
    try:
        root = etree.Element("tv")
        for ch_name, _ in WEIFANG_CHANNELS:
            ch = etree.SubElement(root, "channel", id=ch_name)
            dn = etree.SubElement(ch, "display-name", lang="zh")
            dn.text = ch_name

        for ch_name, base_url in WEIFANG_CHANNELS:
            programs = get_channel_7days(channel_name=ch_name, base_url=base_url)
            for start, stop, title in programs:
                prog = etree.SubElement(root, "programme", start=start, stop=stop, channel=ch_name)
                t = etree.SubElement(prog, "title", lang="zh")
                t.text = title

        wf_path = os.path.join(OUTPUT_DIR, "weifang.gz")
        xml_content = etree.tostring(root, encoding="utf-8", pretty_print=True, xml_declaration=True)
        
        with gzip.open(wf_path, "wb") as f:
            f.write(xml_content)
            
        print(f"âœ… æ½åŠEPGå·²ä¿å­˜: {wf_path}")
        return wf_path
        
    except Exception as e:
        print(f"âŒ æ½åŠæºæŠ“å–å¤±è´¥: {e}")
        wf_path = os.path.join(OUTPUT_DIR, "weifang.gz")
        empty_xml = b'<?xml version="1.0" encoding="utf-8"?>\n<tv></tv>'
        with gzip.open(wf_path, "wb") as f:
            f.write(empty_xml)
        return wf_path

# ====================== ä¿®å¤åçš„åˆå¹¶é€»è¾‘ ======================
def fetch_with_retry(u, max_retry=MAX_RETRY):
    for attempt in range(1, max_retry + 1):
        try:
            r = requests.get(u, timeout=15, headers={"User-Agent": "Mozilla/5.0"})
            if r.status_code not in (200, 206):
                time.sleep(1)
                continue

            if u.endswith(".gz"):
                content = gzip.decompress(r.content).decode("utf-8", "ignore")
            else:
                content = r.text

            # æ¸…æ´—XMLå†…å®¹
            content = clean_xml_content(content)
            
            # å°è¯•è§£æXML
            try:
                parser = etree.XMLParser(recover=True)
                tree = etree.fromstring(content.encode("utf-8"), parser=parser)
                
                # éªŒè¯XMLç»“æ„
                channels = tree.xpath("//channel")
                programmes = tree.xpath("//programme")
                
                if len(channels) > 0 and len(programmes) > 0:
                    return (True, tree, len(channels), len(programmes), attempt)
            except Exception as e:
                print(f"âš ï¸ XMLè§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤: {e}")
                # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•æå–æœ‰æ•ˆå†…å®¹
                channels = re.findall(r'<channel[^>]*>.*?</channel>', content, re.DOTALL)
                programmes = re.findall(r'<programme[^>]*>.*?</programme>', content, re.DOTALL)
                
                if channels and programmes:
                    # æ„å»ºæ–°çš„XML
                    fixed_xml = '<?xml version="1.0" encoding="utf-8"?>\n<tv>\n'
                    fixed_xml += '\n'.join(channels)
                    fixed_xml += '\n'.join(programmes)
                    fixed_xml += '\n</tv>'
                    
                    parser = etree.XMLParser(recover=True)
                    tree = etree.fromstring(fixed_xml.encode("utf-8"), parser=parser)
                    return (True, tree, len(channels), len(programmes), attempt)
                    
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥ {u[:50]}...: {e}")
            time.sleep(1)
            continue
    return (False, None, 0, 0, max_retry)

def merge_all(weifang_gz_file):
    all_channels = []
    all_programs = []
    total_ch = 0
    total_pg = 0
    success_cnt = 0
    fail_cnt = 0

    if not os.path.exists("config.txt"):
        print("âŒ æœªæ‰¾åˆ° config.txt æ–‡ä»¶")
        return

    with open("config.txt", "r", encoding="utf-8") as f:
        urls = [l.strip() for l in f if l.strip() and l.startswith("http")]

    if not urls:
        print("âŒ config.txt ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„URL")
        return

    print("=" * 60)
    print("EPG æºæŠ“å–ç»Ÿè®¡ï¼ˆå¤±è´¥è‡ªåŠ¨é‡è¯•ï¼‰")
    print("=" * 60)

    with ThreadPoolExecutor(max_workers=3) as executor:  # å‡å°‘çº¿ç¨‹æ•°ä»¥é¿å…å†…å­˜é—®é¢˜
        future_map = {executor.submit(fetch_with_retry, u): u for u in urls}
        for fut in future_map:
            u = future_map[fut]
            ok, tree, ch, pg, retry_cnt = fut.result()
            if ok:
                success_cnt += 1
                total_ch += ch
                total_pg += pg
                log_retry = f"[é‡è¯•{retry_cnt-1}æ¬¡]" if retry_cnt > 1 else ""
                print(f"âœ… {u[:55]}... {log_retry}æˆåŠŸ | é¢‘é“ {ch:>4} | èŠ‚ç›® {pg:>6}")
                for node in tree:
                    if node.tag == "channel":
                        all_channels.append(node)
                    elif node.tag == "programme":
                        all_programs.append(node)
            else:
                fail_cnt += 1

    if fail_cnt > 0:
        print(f"âŒ å…± {fail_cnt} ä¸ªæºç»{MAX_RETRY}æ¬¡é‡è¯•åä»å¤±è´¥ï¼Œå·²è·³è¿‡")

    print("=" * 60)
    print(f"æ±‡æ€»ï¼šæˆåŠŸ {success_cnt} ä¸ª | å¤±è´¥ {fail_cnt} ä¸ª | æ€»é¢‘é“ {total_ch} | æ€»èŠ‚ç›® {total_pg}")
    print("=" * 60)

    try:
        with gzip.open(weifang_gz_file, "rb") as f:
            wf_content = f.read().decode("utf-8")
            parser = etree.XMLParser(recover=True)
            wf_tree = etree.fromstring(wf_content.encode("utf-8"), parser)
            wf_ch = len(wf_tree.xpath("//channel"))
            wf_pg = len(wf_tree.xpath("//programme"))

        if wf_ch > 0 and wf_pg > 0:
            print(f"ğŸ“º æ½åŠæœ¬åœ°æºï¼šé¢‘é“ {wf_ch} | èŠ‚ç›® {wf_pg}ï¼ˆæœ¬å‘¨ä¸€~å‘¨æ—¥å®Œæ•´7å¤©ï¼‰")
            for node in wf_tree:
                if node.tag == "channel":
                    all_channels.append(node)
                elif node.tag == "programme":
                    all_programs.append(node)
        else:
            print("âš ï¸ æ½åŠæœ¬åœ°æºæŠ“å–å¤±è´¥ï¼Œå·²è·³è¿‡")
    except Exception as e:
        print(f"âš ï¸ æ½åŠæœ¬åœ°æºè¯»å–å¤±è´¥: {e}")

    print(f"å»é‡å‰: {len(all_channels)} ä¸ªé¢‘é“, {len(all_programs)} ä¸ªèŠ‚ç›®")

    # ====================== é¢‘é“å»é‡ ======================
    seen_channel_names = set()
    unique_channels = []
    channel_id_mapping = {}  # å­˜å‚¨åŸå§‹é¢‘é“IDåˆ°ä¿ç•™é¢‘é“IDçš„æ˜ å°„
    
    for ch in all_channels:
        try:
            display_name_node = ch.find("display-name")
            if display_name_node is not None and display_name_node.text:
                channel_name = display_name_node.text.strip()
                channel_name_lower = channel_name.lower()
                
                # æ¸…ç†é¢‘é“ID
                channel_id = ch.get('id', '')
                
                if channel_name_lower not in seen_channel_names:
                    seen_channel_names.add(channel_name_lower)
                    unique_channels.append(ch)
                    
                    if channel_id:
                        channel_id_mapping[channel_name_lower] = channel_id
                else:
                    if channel_id and channel_name_lower in channel_id_mapping:
                        retained_id = channel_id_mapping[channel_name_lower]
                        channel_id_mapping[channel_id] = retained_id
            else:
                # æ²¡æœ‰display-nameçš„é¢‘é“ï¼Œç›´æ¥ä¿ç•™
                unique_channels.append(ch)
        except Exception as e:
            print(f"âš ï¸ å¤„ç†é¢‘é“æ—¶å‡ºé”™: {e}")
            continue
    
    print(f"å»é‡å: {len(unique_channels)} ä¸ªé¢‘é“")
    
    # ====================== æ¸…ç†èŠ‚ç›®å¹¶æ›´æ–°é¢‘é“ID ======================
    cleaned_programs = []
    for prog in all_programs:
        try:
            # è·å–å’Œæ¸…ç†å±æ€§
            old_channel_id = prog.get('channel', '')
            start = prog.get('start', '')
            stop = prog.get('stop', '')
            
            # æŸ¥æ‰¾æ­£ç¡®çš„é¢‘é“ID
            new_channel_id = old_channel_id
            for old_id, retained_id in channel_id_mapping.items():
                if old_channel_id == old_id:
                    new_channel_id = retained_id
                    break
            
            # æ£€æŸ¥èŠ‚ç›®æ˜¯å¦æœ‰æ•ˆ
            if not start or not stop or not new_channel_id:
                continue
            
            # æ¸…ç†å¼€å§‹å’Œç»“æŸæ—¶é—´æ ¼å¼
            if not re.match(r'^\d{14} \+0800$', start):
                continue
            if not re.match(r'^\d{14} \+0800$', stop):
                continue
            
            # åˆ›å»ºæ–°çš„èŠ‚ç›®å…ƒç´ 
            new_prog = etree.Element("programme", start=start, stop=stop, channel=new_channel_id)
            
            # æ·»åŠ æ ‡é¢˜
            title_elem = prog.find("title")
            if title_elem is not None and title_elem.text:
                title = etree.SubElement(new_prog, "title", lang="zh")
                title.text = title_elem.text.strip()
            else:
                continue
                
            cleaned_programs.append(new_prog)
            
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†èŠ‚ç›®æ—¶å‡ºé”™: {e}")
            continue
    
    all_programs = cleaned_programs
    print(f"æ¸…ç†å: {len(all_programs)} ä¸ªæœ‰æ•ˆèŠ‚ç›®")
    
    # ====================== ç”Ÿæˆæœ€ç»ˆXML ======================
    try:
        final_root = etree.Element("tv")
        
        # æ·»åŠ é¢‘é“
        for ch in unique_channels:
            # ç¡®ä¿é¢‘é“æ ¼å¼æ­£ç¡®
            try:
                final_root.append(ch)
            except Exception as e:
                print(f"âš ï¸ æ·»åŠ é¢‘é“æ—¶å‡ºé”™: {e}")
                continue
        
        # æ·»åŠ èŠ‚ç›®
        for prog in all_programs:
            try:
                final_root.append(prog)
            except Exception as e:
                print(f"âš ï¸ æ·»åŠ èŠ‚ç›®æ—¶å‡ºé”™: {e}")
                continue
        
        # ç”ŸæˆXML
        xml_declaration = '<?xml version="1.0" encoding="utf-8"?>\n'
        xml_str = xml_declaration.encode('utf-8') + etree.tostring(final_root, encoding="utf-8", pretty_print=True)
        
        # æœ€ç»ˆéªŒè¯
        parser = etree.XMLParser(recover=True)
        etree.fromstring(xml_str, parser)
        
        output_path = os.path.join(OUTPUT_DIR, "epg.gz")
        with gzip.open(output_path, "wb") as f:
            f.write(xml_str)
        
        # è®¡ç®—æ–‡ä»¶å¤§å°
        file_size_mb = os.path.getsize(output_path) / 1024 / 1024
        print(f"âœ… æœ€ç»ˆè¾“å‡ºï¼šé¢‘é“ {len(unique_channels)} ä¸ª | èŠ‚ç›® {len(all_programs)} ä¸ª")
        print(f"ğŸ“¦ æ–‡ä»¶å¤§å°ï¼š{file_size_mb:.2f} MB")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶ï¼š{output_path}")
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆæœ€ç»ˆXMLå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        # ç”Ÿæˆæœ€å°å¯ç”¨çš„XML
        output_path = os.path.join(OUTPUT_DIR, "epg.gz")
        empty_xml = b'<?xml version="1.0" encoding="utf-8"?>\n<tv></tv>'
        with gzip.open(output_path, "wb") as f:
            f.write(empty_xml)
        print("âš ï¸ å·²ç”Ÿæˆç©ºçš„EPGæ–‡ä»¶")

    print("=" * 60)

# ====================== å…¥å£ ======================
if __name__ == "__main__":
    try:
        print("å¼€å§‹æŠ“å–æ½åŠæœ¬åœ°EPG...")
        wf_gz = crawl_weifang()
        print("å¼€å§‹åˆå¹¶æ‰€æœ‰EPGæº...")
        merge_all(wf_gz)
        print("âœ… EPGåˆå¹¶å®Œæˆï¼")
    except Exception as e:
        print(f"âŒ è„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
