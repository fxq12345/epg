import requests
import random
import time
import re
import os
from bs4 import Beautifuimport requests
import random
import time
import re
import os
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

# æ½åŠé¢‘é“é…ç½®
weifang_channels = [
    {"id": "SDWF-SDWF1", "name": "æ½åŠæ–°é—»ç»¼åˆé¢‘é“", "url": "https://m.tvmao.com/program/SDWF-SDWF1-w3.html"},
    {"id": "SDWF-SDWF3", "name": "æ½åŠç”Ÿæ´»é¢‘é“", "url": "https://m.tvmao.com/program/SDWF-SDWF3-w3.html"},
    {"id": "SDWF-SDWF2", "name": "æ½åŠå…¬å…±é¢‘é“", "url": "https://m.tvmao.com/program/SDWF-SDWF2-w3.html"},
    {"id": "SDWF-SDWF4", "name": "æ½åŠç§‘æ•™é¢‘é“", "url": "https://m.tvmao.com/program/SDWF-SDWF4-w3.html"}
]

def extract_week_offset(url):
    match = re.search(r'w(\d+)\.html', url)
    return int(match.group(1)) if match else 1

def get_target_date(week_offset):
    today = datetime.now().date()
    return (today + timedelta(days=week_offset - 1)).strftime("%Y-%m-%d")

def crawl_channel_epg(channel):
    epg_data = []
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}
    try:
        week_offset = extract_week_offset(channel["url"])
        calc_date = get_target_date(week_offset)
        print(f"ğŸ“… {channel['name']} - è®¡ç®—æ—¥æœŸï¼š{calc_date}ï¼ˆw{week_offset}ï¼‰")
        
        response = requests.get(channel["url"], headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        
        # ä¿®å¤ï¼šé‡æ–°å®šä½é¡µé¢æ—¥æœŸï¼ˆé€‚é…tvmaoæ–°ç»“æ„ï¼‰
        date_elem = soup.find("div", class_="program_date") or soup.find("h1", class_="program_date")
        use_date = date_elem.text.strip()[:10] if date_elem else calc_date
        print(f"âœ… ä½¿ç”¨æ—¥æœŸï¼š{use_date}")
        
        # ä¿®å¤ï¼šé‡æ–°å®šä½èŠ‚ç›®å…ƒç´ ï¼ˆé€‚é…tvmaoæ–°ç»“æ„ï¼‰
        for item in soup.find_all("li", class_="program_item"):  # æ›¿æ¢ä¸ºli.program_item
            time_elem = item.find("span", class_="time")
            title_elem = item.find("span", class_="name")
            if time_elem and title_elem:
                time_str = time_elem.text.strip()
                title = title_elem.text.strip()
                start_time = datetime.strptime(f"{use_date} {time_str}", "%Y-%m-%d %H:%M")
                epg_data.append({
                    "channel_id": channel["id"],
                    "start": start_time.strftime("%Y%m%d%H%M%S +0800"),
                    "title": title
                })
        
        time.sleep(1 + random.random()*1.5)
        print(f"ğŸ“Š çˆ¬å–å®Œæˆï¼š{len(epg_data)}æ¡èŠ‚ç›®\n")
    except Exception as e:
        print(f"âŒ {channel['name']}çˆ¬å–å¤±è´¥ï¼š{str(e)}\n")
    return epg_data

def generate_xmltv_file(epg_data, channels):
    import xml.etree.ElementTree as ET
    from xml.dom import minidom
    tv = ET.Element("tv", {
        "source-info-url": "https://m.tvmao.com",
        "source-info-name": "TVmao-æ½åŠEPG",
        "generated-date": datetime.now().strftime("%Y%m%d%H%M%S +0800")
    })
    for channel in channels:
        chan_elem = ET.SubElement(tv, "channel", {"id": channel["id"]})
        ET.SubElement(chan_elem, "display-name").text = channel["name"]
    for prog in epg_data:
        prog_elem = ET.SubElement(tv, "programme", {"start": prog["start"], "channel": prog["channel_id"]})
        ET.SubElement(prog_elem, "title", {"lang": "zh"}).text = prog["title"]
    os.makedirs("output", exist_ok=True)
    xml_str = minidom.parseString(ET.tostring(tv)).toprettyxml(indent="  ")
    with open("output/weifang.xml", "w", encoding="utf-8") as f:
        f.write(xml_str)
    print(f"ğŸ‰ æ½åŠEPGç”Ÿæˆï¼šoutput/weifang.xmlï¼ˆ{len(epg_data)}æ¡èŠ‚ç›®ï¼‰")

if __name__ == "__main__":
    all_epg = []
    print("="*60 + "\næ½åŠEPGçˆ¬è™«å¯åŠ¨\n" + "="*60)
    for channel in weifang_channels:
        all_epg.extend(crawl_channel_epg(channel))
    generate_xmltv_file(all_epg, weifang_channels)
lSoup
from datetime import datetime, timedelta
import xml.etree.ElementTree as ET
from xml.dom import minidom

# æ½åŠé¢‘é“é…ç½®ï¼ˆç»‘å®šç²¾å‡†é“¾æ¥ï¼Œåç§°å«â€œæ½åŠâ€é€‚é…merge.pyåˆ†ç±»ï¼‰
weifang_channels = [
    {
        "id": "SDWF-SDWF1",
        "name": "æ½åŠæ–°é—»ç»¼åˆé¢‘é“",
        "url": "https://m.tvmao.com/program/SDWF-SDWF1-w3.html"
    },
    {
        "id": "SDWF-SDWF3",
        "name": "æ½åŠç”Ÿæ´»é¢‘é“",
        "url": "https://m.tvmao.com/program/SDWF-SDWF3-w3.html"
    },
    {
        "id": "SDWF-SDWF2",
        "name": "æ½åŠå…¬å…±é¢‘é“",
        "url": "https://m.tvmao.com/program/SDWF-SDWF2-w3.html"
    },
    {
        "id": "SDWF-SDWF4",
        "name": "æ½åŠç§‘æ•™é¢‘é“",
        "url": "https://m.tvmao.com/program/SDWF-SDWF4-w3.html"
    }
]

# ä»é“¾æ¥æå–å‘¨åç§»é‡ï¼ˆw3â†’3ï¼‰
def extract_week_offset(url):
    match = re.search(r'w(\d+)\.html', url)
    return int(match.group(1)) if match else 1

# è®¡ç®—ç›®æ ‡æ—¥æœŸï¼ˆw1=ä»Šæ—¥ï¼Œw2=æ˜æ—¥...ï¼‰
def get_target_date(week_offset):
    today = datetime.now().date()
    target_date = today + timedelta(days=week_offset - 1)
    return target_date.strftime("%Y-%m-%d")

# çˆ¬å–å•ä¸ªé¢‘é“èŠ‚ç›®å•
def crawl_channel_epg(channel):
    epg_data = []
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    try:
        week_offset = extract_week_offset(channel["url"])
        calc_date = get_target_date(week_offset)
        print(f"ğŸ“… {channel['name']} - è®¡ç®—æ—¥æœŸï¼š{calc_date}ï¼ˆw{week_offset}ï¼‰")
        
        # å‘é€è¯·æ±‚
        response = requests.get(channel["url"], headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        
        # éªŒè¯é¡µé¢æ—¥æœŸ
        date_elem = soup.find("div", class_="program_date")
        if date_elem:
            page_date = date_elem.text.strip()[:10]
            print(f"âœ… é¡µé¢æ—¥æœŸï¼š{page_date}ï¼ˆä½¿ç”¨é¡µé¢æ—¥æœŸï¼‰")
            use_date = page_date
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°é¡µé¢æ—¥æœŸï¼Œä½¿ç”¨è®¡ç®—æ—¥æœŸï¼š{calc_date}")
            use_date = calc_date
        
        # æå–èŠ‚ç›®æ•°æ®
        for item in soup.find_all("div", class_="p_item"):
            time_elem = item.find("span", class_="p_time")
            title_elem = item.find("a", class_="p_name")
            if time_elem and title_elem:
                time_str = time_elem.text.strip()
                title = title_elem.text.strip()
                start_time = datetime.strptime(f"{use_date} {time_str}", "%Y-%m-%d %H:%M")
                epg_data.append({
                    "channel_id": channel["id"],
                    "start": start_time.strftime("%Y%m%d%H%M%S +0800"),
                    "title": title
                })
        
        time.sleep(1 + random.random()*1.5)  # åçˆ¬å»¶æ—¶
        print(f"ğŸ“Š çˆ¬å–å®Œæˆï¼š{len(epg_data)}æ¡èŠ‚ç›®\n")
    except Exception as e:
        print(f"âŒ {channel['name']}çˆ¬å–å¤±è´¥ï¼š{str(e)}\n")
    return epg_data

# ç”Ÿæˆæ ‡å‡†XMLTVæ ¼å¼EPGæ–‡ä»¶ï¼ˆè¾“å‡ºåˆ°outputç›®å½•ï¼Œé€‚é…merge.pyè¯»å–ï¼‰
def generate_xmltv_file(epg_data, channels):
    tv = ET.Element("tv", {
        "source-info-url": "https://m.tvmao.com",
        "source-info-name": "TVmao-æ½åŠä¸“å±EPG",
        "generator-info-name": "æ½åŠEPGè‡ªåŠ¨çˆ¬è™«",
        "generated-date": datetime.now().strftime("%Y%m%d%H%M%S +0800")
    })
    
    # æ·»åŠ é¢‘é“ä¿¡æ¯ï¼ˆIDå’Œåç§°é€‚é…merge.pyçš„åˆ†ç±»ä¸å»é‡é€»è¾‘ï¼‰
    for channel in channels:
        chan_elem = ET.SubElement(tv, "channel", {"id": channel["id"]})
        ET.SubElement(chan_elem, "display-name").text = channel["name"]
        ET.SubElement(chan_elem, "url").text = channel["url"]
    
    # æ·»åŠ èŠ‚ç›®ä¿¡æ¯ï¼ˆæ ¼å¼ç¬¦åˆXMLTVæ ‡å‡†ï¼Œç¡®ä¿merge.pyå¯è§£æï¼‰
    for prog in epg_data:
        prog_elem = ET.SubElement(tv, "programme", {
            "start": prog["start"],
            "channel": prog["channel_id"]
        })
        ET.SubElement(prog_elem, "title", {"lang": "zh"}).text = prog["title"]
    
    # ç¡®ä¿outputç›®å½•å­˜åœ¨ï¼Œé¿å…æŠ¥é”™
    os.makedirs("output", exist_ok=True)
    # æ ¼å¼åŒ–XMLè¾“å‡ºï¼Œä¾¿äºmerge.pyè§£æ
    xml_str = minidom.parseString(ET.tostring(tv)).toprettyxml(indent="  ")
    with open("output/weifang.xml", "w", encoding="utf-8") as f:
        f.write(xml_str)
    print(f"ğŸ‰ æ½åŠEPGç”Ÿæˆå®Œæˆï¼šoutput/weifang.xmlï¼ˆå…±{len(epg_data)}æ¡èŠ‚ç›®ï¼‰")

if __name__ == "__main__":
    all_epg = []
    print("="*60)
    print("æ½åŠEPGè‡ªåŠ¨çˆ¬è™«å¯åŠ¨ï¼ˆæ¯æ—¥è‡ªåŠ¨æ›´æ–°ï¼‰")
    print("="*60 + "\n")
    for channel in weifang_channels:
        all_epg.extend(crawl_channel_epg(channel))
    generate_xmltv_file(all_epg, weifang_channels)
